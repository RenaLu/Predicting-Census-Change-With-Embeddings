{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CensusPrediction_Income.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5QEoXb0vhch",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmm2_PLSXXms",
        "colab_type": "code",
        "outputId": "ebe1a92d-4510-4b44-f051-652bb074da21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 32.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UhiP9qfJOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy import sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xsyi3AX56i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ignite.handlers import ModelCheckpoint, EarlyStopping, TerminateOnNan\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
        "from ignite.contrib.handlers import ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwPy5Lh3CH-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import string\n",
        "import random\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itB2xTKXZm3o",
        "colab_type": "code",
        "outputId": "9c5bba4f-ffd6-4e05-eb6d-a0f225677fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "import locale\n",
        "from locale import atof, atoi\n",
        "locale.setlocale(locale.LC_NUMERIC, '')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF-8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBzBYsqc1OAw",
        "colab_type": "code",
        "outputId": "8bfc4968-ce67-48c1-886d-46ca00bdf69b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmD0YDfwfafe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_idf = True\n",
        "max_features = 2**10\n",
        "pca_components = 2**4\n",
        "DRIVE_PATH = Path('/content/drive/My Drive/Thesis2019/')\n",
        "NEIGHBOURHOOD_PROFILES_PATH = Path('neighbourhood-profiles-2016-csv.csv')\n",
        "NEIGHBOURHOOD_PROFILES_PATH_OLD = Path('neighbourhood-data-2001-2011.xlsx')\n",
        "DATA_DIR = DRIVE_PATH.joinpath('data_stripped')\n",
        "use_cuda = True\n",
        "years = [2011, 2016]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KxXDNKRxRpT",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kAxwuIYYbw3",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgKDOkoxTQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewsVector(Data.Dataset):\n",
        "    \"\"\"Reviews Vector dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          self.data = self.data.cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class CensusVector(Data.Dataset):\n",
        "  def __init__(self, data, reviews_embedding):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        self.reviews_embedding = torch.tensor(reviews_embedding).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            self.data = self.data.cuda()\n",
        "            self.reviews_embedding = self.reviews_embedding.cuda()\n",
        "            \n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return {\"data\": self.data[idx],\n",
        "              \"reviews_embedding\": self.reviews_embedding[idx]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0yvBkQovmDE",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdiWF8NSwfqx",
        "colab_type": "text"
      },
      "source": [
        "## Selecting and Matching Attributes for 2011 and 2016 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5y8dtFYfIe",
        "colab_type": "text"
      },
      "source": [
        "Match census categories from different years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV574nlTv42q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbh_profiles = pd.read_csv(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH))\n",
        "nbh_profiles_2011 = pd.read_excel(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH_OLD), sheet_name='2011')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadRLgh2wKM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2011 = nbh_profiles_2011[(nbh_profiles_2011['Topic']=='Income of households')].drop(columns=['Category', 'Topic', 'City of Toronto'])\n",
        "income_2016 = nbh_profiles[(nbh_profiles['Topic']=='Income of households in 2015')].drop(columns=['_id', 'Category', 'Topic', 'Data Source', 'City of Toronto'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-UmUuLhF5mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_2011 = income_2011[income_2011['Attribute'] == 'Household total income in 2010 of private households'].index\n",
        "idx_2011_2 = income_2011[income_2011['Attribute'] == 'After-tax income of households in 2010 of private households'].index\n",
        "income_2011 = income_2011.loc[idx_2011[0]+1:idx_2011_2[0]-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqkekXKyGH_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_2016 = income_2016[income_2016['Characteristic'] == 'Total - Household total income groups in 2015 for private households - 100% data'].index\n",
        "idx_2016_2 = income_2016[income_2016['Characteristic'] == 'Total - Household after-tax income groups in 2015 for private households - 100% data'].index\n",
        "idx_2016_3 = income_2016[income_2016['Characteristic'] == '  $15,000 to $19,999'].index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krc_wiGbGNZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2016_1 = income_2016.loc[idx_2016[0]+1:idx_2016_2[0]-1]\n",
        "income_2016_1.loc[1] = income_2016.loc[idx_2016_3[0]]\n",
        "\n",
        "income_2016_1 = income_2016.loc[idx_2016[0]+1:idx_2016_2[0]-1]\n",
        "income_2016_1.loc[1] = income_2016.loc[idx_2016_3[0]]\n",
        "\n",
        "income_2016 = income_2016_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF_Acd4qD24j",
        "colab_type": "code",
        "outputId": "f157a21f-728d-46d4-fad1-b8ee95764098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "list(income_2011['Attribue'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  Under $10,000',\n",
              " '  $10,000 to $19,999',\n",
              " '  $20,000 to $29,999',\n",
              " '  $30,000 to $39,999',\n",
              " '  $40,000 to $49,999',\n",
              " '  $50,000 to $59,999',\n",
              " '  $60,000 to $79,999',\n",
              " '  $80,000 to $99,999',\n",
              " '  $100,000 and over']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI9h2O2_LZ4U",
        "colab_type": "code",
        "outputId": "d48b2ac1-fbb8-4b2b-a097-da4f683881aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "list(income_2016['Characteristic'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  Under $5,000',\n",
              " '  $5,000 to $9,999',\n",
              " '  $10,000 to $14,999',\n",
              " '  $20,000 to $24,999',\n",
              " '  $25,000 to $29,999',\n",
              " '  $30,000 to $34,999',\n",
              " '  $35,000 to $39,999',\n",
              " '  $40,000 to $44,999',\n",
              " '  $45,000 to $49,999',\n",
              " '  $50,000 to $59,999',\n",
              " '  $60,000 to $69,999',\n",
              " '  $70,000 to $79,999',\n",
              " '  $80,000 to $89,999',\n",
              " '  $90,000 to $99,999',\n",
              " '  $100,000 and over',\n",
              " '    $200,000 and over',\n",
              " '  $15,000 to $19,999']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXGhkeOHF_bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2011.set_index('Attribute', inplace=True)\n",
        "\n",
        "income_2011.loc['  Under $10,000'] = income_2011.loc['  Under $5,000'] + income_2011.loc['  $5,000 to $9,999']\n",
        "income_2011.loc['  $10,000 to $19,999'] = income_2011.loc['  $10,000 to $14,999'] + income_2011.loc['  $15,000 to $19,999']\n",
        "income_2011.loc['  $100,000 and over'] = income_2011.loc['  $100,000 to $124,999'] + income_2011.loc['  $125,000 to $149,999'] + income_2011.loc['  $150,000 and over']\n",
        "\n",
        "income_2011 = income_2011.reindex(['  Under $10,000', '  $10,000 to $19,999', '  $20,000 to $29,999', '  $30,000 to $39,999', \n",
        "                     '  $40,000 to $49,999', '  $50,000 to $59,999', '  $60,000 to $79,999', '  $80,000 to $99,999', \n",
        "                     '  $100,000 and over'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te0_22JDGJ0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2016.set_index('Characteristic', inplace=True)\n",
        "income_2016 = income_2016.applymap(atoi)\n",
        "\n",
        "income_2016.loc['  Under $10,000'] = income_2016.loc['  Under $5,000'] + income_2016.loc['  $5,000 to $9,999']\n",
        "income_2016.loc['  $10,000 to $19,999'] = income_2016.loc['  $10,000 to $14,999'] + income_2016.loc['  $15,000 to $19,999']\n",
        "income_2016.loc['  $20,000 to $29,999'] = income_2016.loc['  $20,000 to $24,999'] + income_2016.loc['  $25,000 to $29,999']\n",
        "income_2016.loc['  $30,000 to $39,999'] = income_2016.loc['  $30,000 to $34,999'] + income_2016.loc['  $35,000 to $39,999']\n",
        "income_2016.loc['  $40,000 to $49,999'] = income_2016.loc['  $40,000 to $44,999'] + income_2016.loc['  $45,000 to $49,999']\n",
        "income_2016.loc['  $60,000 to $79,999'] = income_2016.loc['  $60,000 to $69,999'] + income_2016.loc['  $70,000 to $79,999']\n",
        "income_2016.loc['  $80,000 to $99,999'] = income_2016.loc['  $80,000 to $89,999'] + income_2016.loc['  $90,000 to $99,999']\n",
        "\n",
        "income_2016 = income_2016.reindex(['  Under $10,000', '  $10,000 to $19,999', '  $20,000 to $29,999', '  $30,000 to $39,999', \n",
        "                     '  $40,000 to $49,999', '  $50,000 to $59,999', '  $60,000 to $79,999', '  $80,000 to $99,999', \n",
        "                     '  $100,000 and over'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxVKNobHGVmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2011 = income_2011.T.sort_index()\n",
        "income_2016 = income_2016.T.sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZYuOztQw5Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned = [income_2011, income_2016]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmSvGTznxLVP",
        "colab_type": "text"
      },
      "source": [
        "## Build Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eco7ClI4GZFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incomes = {}\n",
        "for income in cleaned:\n",
        "  incomes[year] = income.div(income.sum(axis=1), axis=0)\n",
        "  income.to_csv(DRIVE_PATH.joinpath('income_'+str(year)+'.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_q_9zDIxom",
        "colab_type": "text"
      },
      "source": [
        "## Load Built Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAwtOk-zIxCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incomes = {}\n",
        "\n",
        "for year in years:\n",
        "  incomes[year] = pd.read_csv(DRIVE_PATH.joinpath('income_{}.csv'.format(year)))\n",
        "  incomes[year].set_index('Unnamed: 0', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtT3yMRfxfY5",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Census Change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLjbonnLYieg",
        "colab_type": "text"
      },
      "source": [
        "Census Change prediction is evaluated with Mean Total Absolute Error:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBZZj91qKMy",
        "colab_type": "text"
      },
      "source": [
        "$Mean Total Absolute Error = $\n",
        "\n",
        "$\\frac{1}{\\#Neighbourhoods}\\sum_{Neighbourhoods} \\sum_{categories} |Actual Proportion - Predicted Proportion|$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$categories \\in \\{\\text{No degree, High school diploma, Apprenticeship certificate, College diploma, University certificate below bachelor level, University degree above bachelor level}\\}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34dueeBjxz6o",
        "colab_type": "text"
      },
      "source": [
        "### Train-test split\n",
        "\n",
        "Split by neighbourhoods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTuAGFwx4EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_neighbourhoods, test_neighbourhoods = train_test_split(range(0, 140), test_size=0.15)\n",
        "\n",
        "all_trains, all_vals = [], [] #Do random splits to cross validate\n",
        "folds = 5\n",
        "\n",
        "for i in range(folds):\n",
        "  train_neighbourhoods, val_neighbourhoods = train_test_split(train_val_neighbourhoods, test_size=0.30)\n",
        "  all_trains.append(train_neighbourhoods)\n",
        "  all_vals.append(val_neighbourhoods)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9kM4UO-8pA",
        "colab_type": "text"
      },
      "source": [
        "### Baseline-Predicting no change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrDG6hs-_Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_data = incomes[2011].values\n",
        "actual_data = incomes[2016].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YKgu-xE_q-S",
        "colab_type": "code",
        "outputId": "cb212838-e39e-48d1-fa6b-efbd6ba59880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(dummy_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14757192238340344"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wpLJrKG_veR",
        "colab_type": "code",
        "outputId": "a397303c-3373-4519-d36e-32af00e87866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(dummy_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14101081314013955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5675IoiJxhit",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlYfoxBu3S_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Encoder, self).__init__()\n",
        "                \n",
        "        layers_en = OrderedDict()       \n",
        "        for i in range(len(sizes)-1):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_en[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            if i==0:\n",
        "                nn.init.xavier_uniform_(layers_en[layer_name].weight)\n",
        "            layers_en[act_name] = nn.Tanh() #-1 to 1\n",
        "        \n",
        "        self.encoder = nn.Sequential(layers_en)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x) \n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        sizes = sizes[::-1]\n",
        "        \n",
        "        layers_de = OrderedDict()\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        layers_de['sigmoid'] = nn.Sigmoid() #0 to 1\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQbaL7TxjKb",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model and built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEZ_P7n_-3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_all = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_all[year] = pd.read_csv(DRIVE_PATH.joinpath('neighbourhood_reviews_{}.csv'.format(year)))\n",
        "  reviews_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "reviews = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews[year] = ReviewsVector(reviews_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhvmqz3xrNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X44OFzttxs10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_r = [max_features, 2**10, 2**8, pca_components]\n",
        "encoder = Encoder(sizes_r)\n",
        "decoder = Decoder(sizes_r)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGf84g_Oh34S",
        "colab_type": "code",
        "outputId": "9b5407fb-960b-4256-9e6f-461c88a80561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "encoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_encoder2_500.pth'))\n",
        "encoder.eval()\n",
        "encoder.to(torch.device(\"cuda\"))\n",
        "\n",
        "decoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_decoder2_500.pth'))\n",
        "decoder.eval()\n",
        "decoder.to(torch.device(\"cuda\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=16, out_features=256, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
              "    (activation2): Tanh()\n",
              "    (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71OoLpU43hWF",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuCjk_J3lsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import RidgeCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7z_0fK3vee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = RidgeCV(cv=5)\n",
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "delta_embedding = (encoder(reviews[2016].data) - encoder(reviews[2011].data)).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApOZxNV3xtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(delta_embedding[train_val_neighbourhoods], delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6IgNOM03y5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lr.predict(delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQjARYPADLX",
        "colab_type": "code",
        "outputId": "8dd9cb4f-c9ee-41b2-a065-227a9da83a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10074098288154398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLlU9D3QrcW3",
        "colab_type": "code",
        "outputId": "793c6ea7-fa9a-40de-f882-eef0d93d862f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10321286493590617"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5ldoCryItc",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression (Census Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLhG_VEh3cVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder_C(nn.Module):\n",
        "    def __init__(self, sizes, softmax=False):\n",
        "        super(Decoder_C, self).__init__()\n",
        "        sizes = sizes[::-1]\n",
        "        layers_de = OrderedDict()\n",
        "\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        # layers_de['softmax'] = nn.Softmax(dim=1) # row sum to 1\n",
        "        layers_de['tanh'] = nn.Tanh()\n",
        "        if softmax:\n",
        "          layers_de['softmax'] = nn.Softmax(dim=1)\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBXycsVr3Z6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion_c(data, decoded):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    loss = mse_loss(data, decoded)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKQe1WQ_-V0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_decoder(decoder, dataset, train_indices, test_indices, name='edu_2'):\n",
        "  optimizer_de = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "  scheduler_de = optim.lr_scheduler.ReduceLROnPlateau(optimizer_de, 'min', patience=20, min_lr=min_lr, factor=0.1)\n",
        "\n",
        "  def process_function(engine, batch):\n",
        "    decoder.train()\n",
        "    optimizer_de.zero_grad()\n",
        "    decoded = decoder(batch['reviews_embedding'])\n",
        "    loss = criterion_c(decoded, batch['data'])\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_de.step()\n",
        "    return loss.item()\n",
        "  \n",
        "\n",
        "  def eval_function(engine, batch):\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        decoded = decoder(batch['reviews_embedding'])\n",
        "        return decoded, batch['data']\n",
        "  \n",
        "  trainer = Engine(process_function)\n",
        "  train_evaluator = Engine(eval_function)\n",
        "  validation_evaluator = Engine(eval_function)\n",
        "\n",
        "  metric = Loss(criterion_c)\n",
        "  metric.attach(train_evaluator, 'loss')\n",
        "  metric.attach(validation_evaluator, 'loss')\n",
        "\n",
        "  training_losses = []\n",
        "  validation_losses = []\n",
        "\n",
        "  @trainer.on(Events.EPOCH_COMPLETED)\n",
        "  def log_training_results(engine):\n",
        "      train_evaluator.run(train_iterator)\n",
        "      metrics = train_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']    \n",
        "      training_losses.append(avg_loss)\n",
        "      print(\"Training Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "           .format(engine.state.epoch, avg_loss))\n",
        "      \n",
        "  def log_validation_results(engine):\n",
        "      validation_evaluator.run(valid_iterator)\n",
        "      metrics = validation_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']\n",
        "      validation_losses.append(avg_loss)\n",
        "\n",
        "      print(\"Validation Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "            .format(engine.state.epoch, avg_loss))\n",
        "\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\n",
        "\n",
        "  # Reduce on Plateau\n",
        "  def average_loss(engine):\n",
        "    print(\"Current lr: {}\".format(optimizer_de.param_groups[0]['lr']))\n",
        "    average_loss = engine.state.metrics['loss']\n",
        "    scheduler_de.step(average_loss)\n",
        "\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, average_loss)\n",
        "  \n",
        "  # Early Stopping\n",
        "  def score_function(engine):\n",
        "      val_loss = engine.state.metrics['loss']\n",
        "      return -val_loss\n",
        "\n",
        "  handler = EarlyStopping(patience=30, score_function=score_function, trainer=trainer)\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n",
        "\n",
        "  # Model Checkpoint\n",
        "  checkpointer = ModelCheckpoint(str(DRIVE_PATH.joinpath('models')), name, n_saved=1, create_dir=False, save_as_state_dict=True, require_empty=False)\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'decoder_c': decoder})\n",
        "\n",
        "  train_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(train_indices))\n",
        "  valid_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(test_indices))\n",
        "\n",
        "  trainer.run(train_iterator, max_epochs=1000)\n",
        "  return training_losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0G6Xm--bxQ",
        "colab_type": "code",
        "outputId": "ac97d851-f6e6-401c-a361-9cbf5a9ff04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "delta_embedding = encoder(reviews[2016].data) - encoder(reviews[2011].data)\n",
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEsgvF_JH8lW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], pca_components]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKGQL_GIulg",
        "colab_type": "code",
        "outputId": "86df0b79-c374-411e-a8db-e89a5a4c978f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=16, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvBlIbgHOMbH",
        "colab_type": "code",
        "outputId": "340ac084-6885-4fa2-d28c-31a0f17730b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i])\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0052946128\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0057191926\n",
            "Training Results - Epoch: 2  Avg loss: 0.0029022310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0034734008\n",
            "Training Results - Epoch: 3  Avg loss: 0.0018275210\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0024848400\n",
            "Training Results - Epoch: 4  Avg loss: 0.0013567157\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0020110682\n",
            "Training Results - Epoch: 5  Avg loss: 0.0010867487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0017343459\n",
            "Training Results - Epoch: 6  Avg loss: 0.0009131554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0015350160\n",
            "Training Results - Epoch: 7  Avg loss: 0.0007980848\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0013765588\n",
            "Training Results - Epoch: 8  Avg loss: 0.0007130811\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0012541503\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006453837\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0011438526\n",
            "Training Results - Epoch: 10  Avg loss: 0.0005883763\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0010524633\n",
            "Training Results - Epoch: 11  Avg loss: 0.0005449758\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0009656097\n",
            "Training Results - Epoch: 12  Avg loss: 0.0005092962\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0009198063\n",
            "Training Results - Epoch: 13  Avg loss: 0.0004745126\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0008473097\n",
            "Training Results - Epoch: 14  Avg loss: 0.0004517546\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0007900271\n",
            "Training Results - Epoch: 15  Avg loss: 0.0004261522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0007460129\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004003898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0007077248\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003868168\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0006724993\n",
            "Training Results - Epoch: 18  Avg loss: 0.0003650462\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0006289357\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003512773\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0005923822\n",
            "Training Results - Epoch: 20  Avg loss: 0.0003348484\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005614995\n",
            "Training Results - Epoch: 21  Avg loss: 0.0003231769\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0005378535\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003109611\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005140118\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003026244\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004846359\n",
            "Training Results - Epoch: 24  Avg loss: 0.0002917588\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0004611815\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002878218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004612219\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002769993\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0004240673\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002709513\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004277847\n",
            "Training Results - Epoch: 28  Avg loss: 0.0002611674\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0003946932\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002614622\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003975415\n",
            "Training Results - Epoch: 30  Avg loss: 0.0002526504\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003856321\n",
            "Training Results - Epoch: 31  Avg loss: 0.0002481435\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003674244\n",
            "Training Results - Epoch: 32  Avg loss: 0.0002406629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003551058\n",
            "Training Results - Epoch: 33  Avg loss: 0.0002397576\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003550322\n",
            "Training Results - Epoch: 34  Avg loss: 0.0002368020\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003419327\n",
            "Training Results - Epoch: 35  Avg loss: 0.0002328068\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003343916\n",
            "Training Results - Epoch: 36  Avg loss: 0.0002302818\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003330544\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002271234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003232632\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002230579\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003199926\n",
            "Training Results - Epoch: 39  Avg loss: 0.0002223153\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003221726\n",
            "Training Results - Epoch: 40  Avg loss: 0.0002220322\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003144157\n",
            "Training Results - Epoch: 41  Avg loss: 0.0002200533\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003181084\n",
            "Training Results - Epoch: 42  Avg loss: 0.0002221828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003055086\n",
            "Training Results - Epoch: 43  Avg loss: 0.0002172981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003102047\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002183703\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002949140\n",
            "Training Results - Epoch: 45  Avg loss: 0.0002155239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003013164\n",
            "Training Results - Epoch: 46  Avg loss: 0.0002126334\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002991084\n",
            "Training Results - Epoch: 47  Avg loss: 0.0002123185\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002948737\n",
            "Training Results - Epoch: 48  Avg loss: 0.0002096473\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0002933305\n",
            "Training Results - Epoch: 49  Avg loss: 0.0002094828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002925655\n",
            "Training Results - Epoch: 50  Avg loss: 0.0002103975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002870338\n",
            "Training Results - Epoch: 51  Avg loss: 0.0002070773\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0002919133\n",
            "Training Results - Epoch: 52  Avg loss: 0.0002137452\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002852281\n",
            "Training Results - Epoch: 53  Avg loss: 0.0002092096\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0003012667\n",
            "Training Results - Epoch: 54  Avg loss: 0.0002061922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0002868155\n",
            "Training Results - Epoch: 55  Avg loss: 0.0002071724\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002950963\n",
            "Training Results - Epoch: 56  Avg loss: 0.0002075077\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0003012244\n",
            "Training Results - Epoch: 57  Avg loss: 0.0002070648\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0002911031\n",
            "Training Results - Epoch: 58  Avg loss: 0.0002064635\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0002896440\n",
            "Training Results - Epoch: 59  Avg loss: 0.0002157964\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0003105320\n",
            "Training Results - Epoch: 60  Avg loss: 0.0002078456\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0002901960\n",
            "Training Results - Epoch: 61  Avg loss: 0.0002020920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0002830047\n",
            "Training Results - Epoch: 62  Avg loss: 0.0002011457\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0002793719\n",
            "Training Results - Epoch: 63  Avg loss: 0.0002062064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0002869853\n",
            "Training Results - Epoch: 64  Avg loss: 0.0002029981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0002836795\n",
            "Training Results - Epoch: 65  Avg loss: 0.0002016799\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002829547\n",
            "Training Results - Epoch: 66  Avg loss: 0.0002000001\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0002813765\n",
            "Training Results - Epoch: 67  Avg loss: 0.0002019297\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002958087\n",
            "Training Results - Epoch: 68  Avg loss: 0.0002062787\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0002895424\n",
            "Training Results - Epoch: 69  Avg loss: 0.0001983208\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0002876343\n",
            "Training Results - Epoch: 70  Avg loss: 0.0002036984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0002861941\n",
            "Training Results - Epoch: 71  Avg loss: 0.0002037772\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002857400\n",
            "Training Results - Epoch: 72  Avg loss: 0.0001989109\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0002852230\n",
            "Training Results - Epoch: 73  Avg loss: 0.0001995591\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002867901\n",
            "Training Results - Epoch: 74  Avg loss: 0.0002043902\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002838971\n",
            "Training Results - Epoch: 75  Avg loss: 0.0001976577\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002835106\n",
            "Training Results - Epoch: 76  Avg loss: 0.0002032388\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0002957257\n",
            "Training Results - Epoch: 77  Avg loss: 0.0002022715\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0002889288\n",
            "Training Results - Epoch: 78  Avg loss: 0.0002063708\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0002803712\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001987843\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002825009\n",
            "Training Results - Epoch: 80  Avg loss: 0.0002001975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002827219\n",
            "Training Results - Epoch: 81  Avg loss: 0.0002051805\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0002824594\n",
            "Training Results - Epoch: 82  Avg loss: 0.0001981958\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0002862979\n",
            "Training Results - Epoch: 83  Avg loss: 0.0002028522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0003006440\n",
            "Training Results - Epoch: 84  Avg loss: 0.0001962219\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0002898003\n",
            "Training Results - Epoch: 85  Avg loss: 0.0001941722\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0002852076\n",
            "Training Results - Epoch: 86  Avg loss: 0.0001932349\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0002823843\n",
            "Training Results - Epoch: 87  Avg loss: 0.0001930959\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0002820749\n",
            "Training Results - Epoch: 88  Avg loss: 0.0001930044\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0002825436\n",
            "Training Results - Epoch: 89  Avg loss: 0.0001929540\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0002818503\n",
            "Training Results - Epoch: 90  Avg loss: 0.0001928958\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0002819360\n",
            "Training Results - Epoch: 91  Avg loss: 0.0001928407\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0002816277\n",
            "Training Results - Epoch: 92  Avg loss: 0.0001928556\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0002814854\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002094867\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002551727\n",
            "Training Results - Epoch: 2  Avg loss: 0.0002021986\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002639475\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001964550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002539543\n",
            "Training Results - Epoch: 4  Avg loss: 0.0001945250\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002728032\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001953942\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002765290\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001913078\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002797280\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001918087\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002974554\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001934965\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0003005832\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001932254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0003049996\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001869803\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002899385\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001910618\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003065401\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001885861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0003033361\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001925126\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0003044867\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001871281\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0003038481\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001916363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003021185\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001900204\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003152398\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001927686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003221671\n",
            "Training Results - Epoch: 18  Avg loss: 0.0002011893\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0003536549\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001931394\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003320451\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001875976\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003193914\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001919358\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003367557\n",
            "Training Results - Epoch: 22  Avg loss: 0.0002051596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003273722\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001876134\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003344365\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001905554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003406200\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001853639\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0003321398\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001837425\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0003265420\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001834087\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003249723\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001832137\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0003237965\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001831312\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003241648\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001830406\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003239707\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001830064\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003230168\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001829865\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003232188\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001829367\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003234679\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002177585\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0001957925\n",
            "Training Results - Epoch: 2  Avg loss: 0.0002099499\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002012146\n",
            "Training Results - Epoch: 3  Avg loss: 0.0002001329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002080956\n",
            "Training Results - Epoch: 4  Avg loss: 0.0001966304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002192460\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001981018\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002313718\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001934688\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002372895\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001939614\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002512174\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001891067\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002394024\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001901147\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002457901\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001896836\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002565205\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001885567\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002631117\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001977700\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002739350\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001901042\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002653396\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001861980\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002744440\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001863550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002640174\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001855318\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002611690\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001848151\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002840006\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001915814\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002974005\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001814292\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002775793\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001855080\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002822264\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001878789\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002777933\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001821954\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002772343\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001781877\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002768749\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001770559\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002773026\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001764355\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002774741\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001761593\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002789097\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001759335\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002782116\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001757840\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002792589\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001756913\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002794942\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001755806\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002790506\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001755302\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002795636\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002054741\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002266416\n",
            "Training Results - Epoch: 2  Avg loss: 0.0001977977\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002359701\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001946735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002375631\n",
            "Training Results - Epoch: 4  Avg loss: 0.0002037516\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002391410\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001907546\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002361886\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001871035\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002459944\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001870620\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002445699\n",
            "Training Results - Epoch: 8  Avg loss: 0.0002016270\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002594783\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001870506\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002569341\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001876346\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002625377\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001875882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002637476\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001857454\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002613294\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001881622\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002680022\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001884460\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002723787\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001854069\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002685303\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001870428\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002766689\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001875453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002789263\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001890842\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002788697\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001823832\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002745321\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001898124\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002813505\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001840422\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002912102\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001866466\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002999109\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001820221\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002909004\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001807414\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002871485\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001803542\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002858267\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001801285\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002851210\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001800018\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002858396\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001799113\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002849362\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001799753\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002852393\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001798349\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002850822\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001798276\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002864075\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002027016\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002280282\n",
            "Training Results - Epoch: 2  Avg loss: 0.0001985223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002427275\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001982514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002563611\n",
            "Training Results - Epoch: 4  Avg loss: 0.0002051425\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002889353\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001966821\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002684687\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001944337\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002635456\n",
            "Training Results - Epoch: 7  Avg loss: 0.0002010846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002834711\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001965760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002662093\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001944053\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002584180\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001923015\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002727979\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001982391\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002801015\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001928223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002723756\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001928239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002746057\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001903222\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002687639\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001943051\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002732164\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001960788\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002778951\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001972052\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002610001\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001927216\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002711317\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001934876\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002796012\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001976686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002851768\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001948905\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002890343\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001983711\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002751579\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001901885\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002701317\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001872876\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002702244\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001862715\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002714824\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001857836\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002732985\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001855589\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002745820\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001854859\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002753321\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001854364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002751153\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001854287\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002761666\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001853928\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002752899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVsY58vBYt1u",
        "colab_type": "code",
        "outputId": "654b0eca-eae9-4793-b067-cd22e0f55fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.savefig(DRIVE_PATH.joinpath('loss.png'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICKSUAoAEZGEUgCIiCTU/wfRvEq8QbnucgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b74646a7-1c8f-466f-f89a-6ce671fa4c4b",
        "id": "MNGu-CraHZuT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cMq3lkm-zGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "actual_data = incomes[2016].values\n",
        "predicted_data = incomes[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj1wJE4NtbQG",
        "colab_type": "code",
        "outputId": "5c461d95-e8ef-4b8e-bee8-43dc5c5a2c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09893043176503016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh_2u83Gt7yq",
        "colab_type": "code",
        "outputId": "1e344a08-aa05-4ced-9b44-848503f32fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1290547679091938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc7Acl_o_q8Y",
        "colab_type": "text"
      },
      "source": [
        "## ELMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBVwfdBT_xAg",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model and built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNGkUzM_9Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_elmo_all = {}\n",
        "reviews_elmo = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_elmo_all[year] = pd.read_csv(DRIVE_PATH.joinpath('elmo_reviews_{}.csv'.format(year)))\n",
        "  reviews_elmo_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "  reviews_elmo[year] = ReviewsVector(reviews_elmo_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkrab33AUoX",
        "colab_type": "code",
        "outputId": "3d69b9f5-4b8b-4452-f082-bf1680c1b23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "delta_embedding = reviews_elmo[2016].data.T - reviews_elmo[2011].data.T\n",
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtFEv6GZAMBR",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK-0krFfAPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lingreg = RidgeCV(cv=5)\n",
        "lingreg.fit(delta_embedding[train_val_neighbourhoods].detach().cpu().numpy(), delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3Pkd9OIeax",
        "colab_type": "code",
        "outputId": "4bbed45c-e92e-4ea5-b792-820fd7c83806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09775755647610872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3zzChz7MP-w",
        "colab_type": "code",
        "outputId": "77c17f8e-8f79-4d0c-efff-15d3a269a5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10627870847900157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-NPDm4AooY",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUFcqugAwjZ",
        "colab_type": "text"
      },
      "source": [
        "#### With one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wksmfZieA1ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3OvtNXeA2mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjzEumZjJxmY",
        "colab_type": "code",
        "outputId": "6ee36b76-c1c2-4600-ef3d-2d13342278eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swv9W-tiB4As",
        "colab_type": "code",
        "outputId": "02001ed6-7919-4812-f29c-a1ab5224f593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='inc_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0027470749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0017631822\n",
            "Training Results - Epoch: 2  Avg loss: 0.0033430097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0010259317\n",
            "Training Results - Epoch: 3  Avg loss: 0.0057298000\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0024721965\n",
            "Training Results - Epoch: 4  Avg loss: 0.0062040288\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0011419559\n",
            "Training Results - Epoch: 5  Avg loss: 0.0042454136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009794299\n",
            "Training Results - Epoch: 6  Avg loss: 0.0045418475\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0012158749\n",
            "Training Results - Epoch: 7  Avg loss: 0.0046741401\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008619955\n",
            "Training Results - Epoch: 8  Avg loss: 0.0050186254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0019986758\n",
            "Training Results - Epoch: 9  Avg loss: 0.0051857487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0011316098\n",
            "Training Results - Epoch: 10  Avg loss: 0.0034887220\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0041099784\n",
            "Training Results - Epoch: 11  Avg loss: 0.0035717726\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0006159839\n",
            "Training Results - Epoch: 12  Avg loss: 0.0073471845\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0073531242\n",
            "Training Results - Epoch: 13  Avg loss: 0.0040450547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0005303867\n",
            "Training Results - Epoch: 14  Avg loss: 0.0043873650\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0013052406\n",
            "Training Results - Epoch: 15  Avg loss: 0.0049995329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0015400940\n",
            "Training Results - Epoch: 16  Avg loss: 0.0028425197\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0007094704\n",
            "Training Results - Epoch: 17  Avg loss: 0.0030658290\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0007827909\n",
            "Training Results - Epoch: 18  Avg loss: 0.0018407251\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004745740\n",
            "Training Results - Epoch: 19  Avg loss: 0.0030096423\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0007880414\n",
            "Training Results - Epoch: 20  Avg loss: 0.0054593232\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0031611070\n",
            "Training Results - Epoch: 21  Avg loss: 0.0050706162\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0008985772\n",
            "Training Results - Epoch: 22  Avg loss: 0.0038950780\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0009247338\n",
            "Training Results - Epoch: 23  Avg loss: 0.0042846898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0007187241\n",
            "Training Results - Epoch: 24  Avg loss: 0.0036965364\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0009478626\n",
            "Training Results - Epoch: 25  Avg loss: 0.0030413481\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004959910\n",
            "Training Results - Epoch: 26  Avg loss: 0.0087859000\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0057513464\n",
            "Training Results - Epoch: 27  Avg loss: 0.0046883210\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0007806867\n",
            "Training Results - Epoch: 28  Avg loss: 0.0021245294\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0007142361\n",
            "Training Results - Epoch: 29  Avg loss: 0.0018260615\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0005851798\n",
            "Training Results - Epoch: 30  Avg loss: 0.0031159244\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0008475468\n",
            "Training Results - Epoch: 31  Avg loss: 0.0069715300\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0068732702\n",
            "Training Results - Epoch: 32  Avg loss: 0.0031779636\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003932421\n",
            "Training Results - Epoch: 33  Avg loss: 0.0037694117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0013796252\n",
            "Training Results - Epoch: 34  Avg loss: 0.0031547303\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0004526401\n",
            "Training Results - Epoch: 35  Avg loss: 0.0115309734\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0091489470\n",
            "Training Results - Epoch: 36  Avg loss: 0.0036898863\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0007829980\n",
            "Training Results - Epoch: 37  Avg loss: 0.0034295439\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0006698558\n",
            "Training Results - Epoch: 38  Avg loss: 0.0032897945\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0009184214\n",
            "Training Results - Epoch: 39  Avg loss: 0.0031645254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0005282304\n",
            "Training Results - Epoch: 40  Avg loss: 0.0066140522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0055136000\n",
            "Training Results - Epoch: 41  Avg loss: 0.0044871956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0008973163\n",
            "Training Results - Epoch: 42  Avg loss: 0.0054596386\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0024391679\n",
            "Training Results - Epoch: 43  Avg loss: 0.0080726436\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0067576609\n",
            "Training Results - Epoch: 44  Avg loss: 0.0032912948\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0006967921\n",
            "Training Results - Epoch: 45  Avg loss: 0.0050966962\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0017902183\n",
            "Training Results - Epoch: 46  Avg loss: 0.0039716931\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0011306985\n",
            "Training Results - Epoch: 47  Avg loss: 0.0032618937\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0008213299\n",
            "Training Results - Epoch: 48  Avg loss: 0.0075039956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0053099532\n",
            "Training Results - Epoch: 49  Avg loss: 0.0054821717\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0011759359\n",
            "Training Results - Epoch: 50  Avg loss: 0.0016138917\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0003496226\n",
            "Training Results - Epoch: 51  Avg loss: 0.0043340653\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0016940233\n",
            "Training Results - Epoch: 52  Avg loss: 0.0034625549\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0005873900\n",
            "Training Results - Epoch: 53  Avg loss: 0.0056862663\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0014828282\n",
            "Training Results - Epoch: 54  Avg loss: 0.0061414499\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0026830865\n",
            "Training Results - Epoch: 55  Avg loss: 0.0047109819\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0012133954\n",
            "Training Results - Epoch: 56  Avg loss: 0.0069823566\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0067206004\n",
            "Training Results - Epoch: 57  Avg loss: 0.0041574048\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0007673631\n",
            "Training Results - Epoch: 58  Avg loss: 0.0037351709\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0016029159\n",
            "Training Results - Epoch: 59  Avg loss: 0.0023454963\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0004547865\n",
            "Training Results - Epoch: 60  Avg loss: 0.0032852423\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0009179229\n",
            "Training Results - Epoch: 61  Avg loss: 0.0033694007\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0011810280\n",
            "Training Results - Epoch: 62  Avg loss: 0.0039150032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0016636107\n",
            "Training Results - Epoch: 63  Avg loss: 0.0051263540\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0015749608\n",
            "Training Results - Epoch: 64  Avg loss: 0.0052822305\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0023122608\n",
            "Training Results - Epoch: 65  Avg loss: 0.0042128402\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0007907598\n",
            "Training Results - Epoch: 66  Avg loss: 0.0049749979\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0013108775\n",
            "Training Results - Epoch: 67  Avg loss: 0.0052697866\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0011271338\n",
            "Training Results - Epoch: 68  Avg loss: 0.0040547204\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0016732011\n",
            "Training Results - Epoch: 69  Avg loss: 0.0024720788\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0008902365\n",
            "Training Results - Epoch: 70  Avg loss: 0.0055855651\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0045196156\n",
            "Training Results - Epoch: 71  Avg loss: 0.0058684539\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0012264613\n",
            "Training Results - Epoch: 72  Avg loss: 0.0040010084\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0009964701\n",
            "Training Results - Epoch: 73  Avg loss: 0.0024851091\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0007904117\n",
            "Training Results - Epoch: 74  Avg loss: 0.0020398898\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0005317178\n",
            "Training Results - Epoch: 75  Avg loss: 0.0008267771\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0006968775\n",
            "Training Results - Epoch: 76  Avg loss: 0.0004747845\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0005756957\n",
            "Training Results - Epoch: 77  Avg loss: 0.0003086412\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0005230666\n",
            "Training Results - Epoch: 78  Avg loss: 0.0002287636\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0004712618\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001909447\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0004382482\n",
            "Training Results - Epoch: 80  Avg loss: 0.0001735003\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0004335531\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0030453553\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0006530937\n",
            "Training Results - Epoch: 2  Avg loss: 0.0062713940\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0018987502\n",
            "Training Results - Epoch: 3  Avg loss: 0.0039372829\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0009195629\n",
            "Training Results - Epoch: 4  Avg loss: 0.0033283342\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0012542669\n",
            "Training Results - Epoch: 5  Avg loss: 0.0047631169\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0012619548\n",
            "Training Results - Epoch: 6  Avg loss: 0.0069245846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0043002951\n",
            "Training Results - Epoch: 7  Avg loss: 0.0091345524\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0099154198\n",
            "Training Results - Epoch: 8  Avg loss: 0.0067072553\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0020649011\n",
            "Training Results - Epoch: 9  Avg loss: 0.0149057740\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0163444956\n",
            "Training Results - Epoch: 10  Avg loss: 0.0044557114\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0014500548\n",
            "Training Results - Epoch: 11  Avg loss: 0.0034574774\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0012440901\n",
            "Training Results - Epoch: 12  Avg loss: 0.0036413673\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0008788936\n",
            "Training Results - Epoch: 13  Avg loss: 0.0029945670\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0006316047\n",
            "Training Results - Epoch: 14  Avg loss: 0.0026186743\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0005245995\n",
            "Training Results - Epoch: 15  Avg loss: 0.0030420966\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0006280362\n",
            "Training Results - Epoch: 16  Avg loss: 0.0049312444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0018181235\n",
            "Training Results - Epoch: 17  Avg loss: 0.0025607687\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0004944680\n",
            "Training Results - Epoch: 18  Avg loss: 0.0078647066\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0108727903\n",
            "Training Results - Epoch: 19  Avg loss: 0.0033410333\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0008328759\n",
            "Training Results - Epoch: 20  Avg loss: 0.0024941407\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005497324\n",
            "Training Results - Epoch: 21  Avg loss: 0.0026060863\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0008407458\n",
            "Training Results - Epoch: 22  Avg loss: 0.0028515554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005208068\n",
            "Training Results - Epoch: 23  Avg loss: 0.0034269799\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0011329324\n",
            "Training Results - Epoch: 24  Avg loss: 0.0089048331\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0101406094\n",
            "Training Results - Epoch: 25  Avg loss: 0.0040426398\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0016112910\n",
            "Training Results - Epoch: 26  Avg loss: 0.0127008647\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0142824286\n",
            "Training Results - Epoch: 27  Avg loss: 0.0045359369\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0008754329\n",
            "Training Results - Epoch: 28  Avg loss: 0.0036282271\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0008098663\n",
            "Training Results - Epoch: 29  Avg loss: 0.0028843444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0005997395\n",
            "Training Results - Epoch: 30  Avg loss: 0.0028912102\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0004618465\n",
            "Training Results - Epoch: 31  Avg loss: 0.0028645643\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0018429412\n",
            "Training Results - Epoch: 32  Avg loss: 0.0105930503\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0106688036\n",
            "Training Results - Epoch: 33  Avg loss: 0.0094607595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0069255672\n",
            "Training Results - Epoch: 34  Avg loss: 0.0018380279\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003821061\n",
            "Training Results - Epoch: 35  Avg loss: 0.0031760258\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0016757367\n",
            "Training Results - Epoch: 36  Avg loss: 0.0041283956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0014804193\n",
            "Training Results - Epoch: 37  Avg loss: 0.0045440921\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0009679632\n",
            "Training Results - Epoch: 38  Avg loss: 0.0080316794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0107540631\n",
            "Training Results - Epoch: 39  Avg loss: 0.0086922528\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0066249190\n",
            "Training Results - Epoch: 40  Avg loss: 0.0048058880\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0014174354\n",
            "Training Results - Epoch: 41  Avg loss: 0.0028091964\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0005064272\n",
            "Training Results - Epoch: 42  Avg loss: 0.0041178389\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0026193268\n",
            "Training Results - Epoch: 43  Avg loss: 0.0030977626\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0007955147\n",
            "Training Results - Epoch: 44  Avg loss: 0.0042290105\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0018847768\n",
            "Training Results - Epoch: 45  Avg loss: 0.0079786531\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0054924956\n",
            "Training Results - Epoch: 46  Avg loss: 0.0039844309\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0008422997\n",
            "Training Results - Epoch: 47  Avg loss: 0.0036453027\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0010589937\n",
            "Training Results - Epoch: 48  Avg loss: 0.0074933547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0090623196\n",
            "Training Results - Epoch: 49  Avg loss: 0.0031859853\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0004407763\n",
            "Training Results - Epoch: 50  Avg loss: 0.0066013654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0029797302\n",
            "Training Results - Epoch: 51  Avg loss: 0.0046596341\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0036507977\n",
            "Training Results - Epoch: 52  Avg loss: 0.0025704812\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0014035955\n",
            "Training Results - Epoch: 53  Avg loss: 0.0017371408\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0013207158\n",
            "Training Results - Epoch: 54  Avg loss: 0.0028221274\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0010479526\n",
            "Training Results - Epoch: 55  Avg loss: 0.0051496922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0018244878\n",
            "Training Results - Epoch: 56  Avg loss: 0.0033552658\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0016718112\n",
            "Training Results - Epoch: 57  Avg loss: 0.0017240822\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0011244553\n",
            "Training Results - Epoch: 58  Avg loss: 0.0009039843\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0008560507\n",
            "Training Results - Epoch: 59  Avg loss: 0.0005348600\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0006848862\n",
            "Training Results - Epoch: 60  Avg loss: 0.0002829524\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0006096136\n",
            "Training Results - Epoch: 61  Avg loss: 0.0001777156\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0005300009\n",
            "Training Results - Epoch: 62  Avg loss: 0.0001269284\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0004340192\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001041707\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0003949812\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000941690\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0003721870\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000892599\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0003592875\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000863311\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0003652838\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000839316\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0003573865\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000820506\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0003603760\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000804998\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0003585431\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000791897\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0003570277\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000778166\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0003561957\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000766939\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0003558577\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000756461\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0003500792\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000749468\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0003501082\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000745293\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0003592146\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000735376\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0003532503\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000728801\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0003546357\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000723154\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0003478181\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000715725\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0003488810\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000715211\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0003520584\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000714742\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0003401468\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000706777\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0003545023\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000700648\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0003432002\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000696849\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0003491890\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000699160\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0003411937\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000688280\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0003482100\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000687886\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0003528648\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000680746\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0003420039\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000683345\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0003454772\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000681364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0003440831\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000691594\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0003559454\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000672465\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0003503584\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000679087\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0003484356\n",
            "Training Results - Epoch: 94  Avg loss: 0.0000663129\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0003473243\n",
            "Training Results - Epoch: 95  Avg loss: 0.0000668654\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0003415387\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000671675\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0003471791\n",
            "Training Results - Epoch: 97  Avg loss: 0.0000673260\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0003563190\n",
            "Training Results - Epoch: 98  Avg loss: 0.0000694054\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 98  Avg loss: 0.0003355240\n",
            "Training Results - Epoch: 99  Avg loss: 0.0000687401\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 99  Avg loss: 0.0003592692\n",
            "Training Results - Epoch: 100  Avg loss: 0.0000667983\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 100  Avg loss: 0.0003450571\n",
            "Training Results - Epoch: 101  Avg loss: 0.0000694598\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 101  Avg loss: 0.0003675118\n",
            "Training Results - Epoch: 102  Avg loss: 0.0000655397\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 102  Avg loss: 0.0003467021\n",
            "Training Results - Epoch: 103  Avg loss: 0.0000677339\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 103  Avg loss: 0.0003589524\n",
            "Training Results - Epoch: 104  Avg loss: 0.0000660676\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 104  Avg loss: 0.0003350228\n",
            "Training Results - Epoch: 105  Avg loss: 0.0000685371\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 105  Avg loss: 0.0003592942\n",
            "Training Results - Epoch: 106  Avg loss: 0.0000725803\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 106  Avg loss: 0.0003255201\n",
            "Training Results - Epoch: 107  Avg loss: 0.0000686444\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 107  Avg loss: 0.0003538083\n",
            "Training Results - Epoch: 108  Avg loss: 0.0000919582\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 108  Avg loss: 0.0003413938\n",
            "Training Results - Epoch: 109  Avg loss: 0.0000728351\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 109  Avg loss: 0.0003483343\n",
            "Training Results - Epoch: 110  Avg loss: 0.0000724575\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 110  Avg loss: 0.0003619439\n",
            "Training Results - Epoch: 111  Avg loss: 0.0000777192\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 111  Avg loss: 0.0003423285\n",
            "Training Results - Epoch: 112  Avg loss: 0.0000949124\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 112  Avg loss: 0.0003590333\n",
            "Training Results - Epoch: 113  Avg loss: 0.0001085266\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 113  Avg loss: 0.0003564966\n",
            "Training Results - Epoch: 114  Avg loss: 0.0001939027\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 114  Avg loss: 0.0003610990\n",
            "Training Results - Epoch: 115  Avg loss: 0.0002770722\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 115  Avg loss: 0.0005662799\n",
            "Training Results - Epoch: 116  Avg loss: 0.0001723658\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 116  Avg loss: 0.0004013801\n",
            "Training Results - Epoch: 117  Avg loss: 0.0002209147\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 117  Avg loss: 0.0006297185\n",
            "Training Results - Epoch: 118  Avg loss: 0.0001418500\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 118  Avg loss: 0.0003705452\n",
            "Training Results - Epoch: 119  Avg loss: 0.0001372487\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 119  Avg loss: 0.0003773692\n",
            "Training Results - Epoch: 120  Avg loss: 0.0001308143\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 120  Avg loss: 0.0004988674\n",
            "Training Results - Epoch: 121  Avg loss: 0.0000719345\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 121  Avg loss: 0.0003633019\n",
            "Training Results - Epoch: 122  Avg loss: 0.0000719418\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 122  Avg loss: 0.0003670610\n",
            "Training Results - Epoch: 123  Avg loss: 0.0000698124\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 123  Avg loss: 0.0003567356\n",
            "Training Results - Epoch: 124  Avg loss: 0.0000700868\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 124  Avg loss: 0.0003721521\n",
            "Training Results - Epoch: 125  Avg loss: 0.0000881591\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 125  Avg loss: 0.0004434033\n",
            "Training Results - Epoch: 126  Avg loss: 0.0000919342\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 126  Avg loss: 0.0003800756\n",
            "Training Results - Epoch: 127  Avg loss: 0.0000702200\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 127  Avg loss: 0.0003713089\n",
            "Training Results - Epoch: 128  Avg loss: 0.0000629941\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 128  Avg loss: 0.0003800176\n",
            "Training Results - Epoch: 129  Avg loss: 0.0000611746\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 129  Avg loss: 0.0003734704\n",
            "Training Results - Epoch: 130  Avg loss: 0.0000601909\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 130  Avg loss: 0.0003706851\n",
            "Training Results - Epoch: 131  Avg loss: 0.0000595390\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 131  Avg loss: 0.0003693766\n",
            "Training Results - Epoch: 132  Avg loss: 0.0000590092\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 132  Avg loss: 0.0003681479\n",
            "Training Results - Epoch: 133  Avg loss: 0.0000586733\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 133  Avg loss: 0.0003672430\n",
            "Training Results - Epoch: 134  Avg loss: 0.0000584570\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 134  Avg loss: 0.0003667800\n",
            "Training Results - Epoch: 135  Avg loss: 0.0000582219\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 135  Avg loss: 0.0003658599\n",
            "Training Results - Epoch: 136  Avg loss: 0.0000580453\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 136  Avg loss: 0.0003660521\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0023674824\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0004276795\n",
            "Training Results - Epoch: 2  Avg loss: 0.0046861512\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0014475648\n",
            "Training Results - Epoch: 3  Avg loss: 0.0083893361\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0040739791\n",
            "Training Results - Epoch: 4  Avg loss: 0.0025650237\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0005910692\n",
            "Training Results - Epoch: 5  Avg loss: 0.0031741817\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009383723\n",
            "Training Results - Epoch: 6  Avg loss: 0.0049190565\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0014449853\n",
            "Training Results - Epoch: 7  Avg loss: 0.0028074483\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0006218661\n",
            "Training Results - Epoch: 8  Avg loss: 0.0035849180\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0010704267\n",
            "Training Results - Epoch: 9  Avg loss: 0.0035288293\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0010051749\n",
            "Training Results - Epoch: 10  Avg loss: 0.0040425864\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0009506788\n",
            "Training Results - Epoch: 11  Avg loss: 0.0042620696\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0015524308\n",
            "Training Results - Epoch: 12  Avg loss: 0.0031419658\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0005926784\n",
            "Training Results - Epoch: 13  Avg loss: 0.0046635885\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0019638987\n",
            "Training Results - Epoch: 14  Avg loss: 0.0052468909\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0015551915\n",
            "Training Results - Epoch: 15  Avg loss: 0.0088841547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0058038907\n",
            "Training Results - Epoch: 16  Avg loss: 0.0027983810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005662047\n",
            "Training Results - Epoch: 17  Avg loss: 0.0065930627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0039343363\n",
            "Training Results - Epoch: 18  Avg loss: 0.0030841678\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0005145606\n",
            "Training Results - Epoch: 19  Avg loss: 0.0049502885\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0019417168\n",
            "Training Results - Epoch: 20  Avg loss: 0.0080568239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0055981708\n",
            "Training Results - Epoch: 21  Avg loss: 0.0116010165\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0084748874\n",
            "Training Results - Epoch: 22  Avg loss: 0.0090951217\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0065144592\n",
            "Training Results - Epoch: 23  Avg loss: 0.0016987950\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0008366724\n",
            "Training Results - Epoch: 24  Avg loss: 0.0004425013\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003153362\n",
            "Training Results - Epoch: 25  Avg loss: 0.0003283558\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002849163\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002553813\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002641281\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002159709\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002490566\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001829155\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002373243\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001675373\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002356239\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001408112\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002243005\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001269387\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002202317\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001135720\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0002191729\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001054846\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002183335\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000963185\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002140875\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000909950\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0002116242\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000875660\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0002105467\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000806717\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002085039\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000780734\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002080720\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000748342\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002073949\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000735106\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0002077465\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000713287\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002057164\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000699865\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002050145\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000688145\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002043390\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000678791\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002029576\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000665456\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0002021521\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000659483\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002031861\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000652694\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002001265\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000648795\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0002017473\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000654514\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002013162\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000651599\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002040762\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000633235\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001983237\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000637250\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002013595\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000622009\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001984867\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000620814\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001977130\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000618888\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002018096\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000613491\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001996097\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000608811\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001962053\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000617351\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001958381\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000603546\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001966389\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000603984\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001993364\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000601430\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001973798\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000599190\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001984175\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000602319\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001979862\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000600877\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001997152\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000611575\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002041072\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000606258\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001982397\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000588505\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001992222\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000627747\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0002058658\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000621266\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0002103128\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000596163\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001968706\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000600929\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002051456\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000606677\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0002026293\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000763491\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002214148\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000772537\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002055137\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000940310\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002096984\n",
            "Training Results - Epoch: 76  Avg loss: 0.0001052271\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0002065794\n",
            "Training Results - Epoch: 77  Avg loss: 0.0001465505\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0002378338\n",
            "Training Results - Epoch: 78  Avg loss: 0.0001176764\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001996753\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001251085\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002227002\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000963842\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002129942\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000800128\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0002088741\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000699505\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0002054526\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000638678\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0002026683\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000602251\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0002022680\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000579224\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0002009584\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000565909\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0002010511\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000557198\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0002010392\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000552770\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0002004824\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0007606330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0006288809\n",
            "Training Results - Epoch: 2  Avg loss: 0.0011372123\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0007748641\n",
            "Training Results - Epoch: 3  Avg loss: 0.0007613184\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0005618017\n",
            "Training Results - Epoch: 4  Avg loss: 0.0007621417\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0007962179\n",
            "Training Results - Epoch: 5  Avg loss: 0.0004786735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009893086\n",
            "Training Results - Epoch: 6  Avg loss: 0.0007618510\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0004977413\n",
            "Training Results - Epoch: 7  Avg loss: 0.0006948979\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0007406023\n",
            "Training Results - Epoch: 8  Avg loss: 0.0015811024\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0022735553\n",
            "Training Results - Epoch: 9  Avg loss: 0.0009424007\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0003555431\n",
            "Training Results - Epoch: 10  Avg loss: 0.0014218178\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0016796218\n",
            "Training Results - Epoch: 11  Avg loss: 0.0005918409\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0017101219\n",
            "Training Results - Epoch: 12  Avg loss: 0.0010056317\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0007146167\n",
            "Training Results - Epoch: 13  Avg loss: 0.0014764204\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004663439\n",
            "Training Results - Epoch: 14  Avg loss: 0.0008712088\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0004359993\n",
            "Training Results - Epoch: 15  Avg loss: 0.0008099885\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0011137803\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004517633\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003635233\n",
            "Training Results - Epoch: 17  Avg loss: 0.0006555189\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0007687426\n",
            "Training Results - Epoch: 18  Avg loss: 0.0008979729\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004961604\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001911577\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002585073\n",
            "Training Results - Epoch: 20  Avg loss: 0.0002247579\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003475017\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001858808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003726950\n",
            "Training Results - Epoch: 22  Avg loss: 0.0004713695\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003414408\n",
            "Training Results - Epoch: 23  Avg loss: 0.0016113191\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0011991414\n",
            "Training Results - Epoch: 24  Avg loss: 0.0004438136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0006392810\n",
            "Training Results - Epoch: 25  Avg loss: 0.0005522676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0007694451\n",
            "Training Results - Epoch: 26  Avg loss: 0.0014837707\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0017065849\n",
            "Training Results - Epoch: 27  Avg loss: 0.0007634211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004344130\n",
            "Training Results - Epoch: 28  Avg loss: 0.0019863225\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0014996240\n",
            "Training Results - Epoch: 29  Avg loss: 0.0007074431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0010504216\n",
            "Training Results - Epoch: 30  Avg loss: 0.0042162596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0058345523\n",
            "Training Results - Epoch: 31  Avg loss: 0.0013521089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0023767901\n",
            "Training Results - Epoch: 32  Avg loss: 0.0021557235\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0030002946\n",
            "Training Results - Epoch: 33  Avg loss: 0.0010774272\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0008362428\n",
            "Training Results - Epoch: 34  Avg loss: 0.0016258728\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0008594430\n",
            "Training Results - Epoch: 35  Avg loss: 0.0006948059\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0008302456\n",
            "Training Results - Epoch: 36  Avg loss: 0.0002993239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003035930\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002272716\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002940411\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002671573\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003648820\n",
            "Training Results - Epoch: 39  Avg loss: 0.0005222705\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003105169\n",
            "Training Results - Epoch: 40  Avg loss: 0.0005945639\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0010492097\n",
            "Training Results - Epoch: 41  Avg loss: 0.0002861465\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0006592534\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001792029\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003886902\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001286936\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003163399\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001057799\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002453436\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000894842\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0002551548\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000797535\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002444795\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000730178\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002372269\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000697405\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0002277976\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000650134\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002472433\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000615370\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002173167\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000605777\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0002506821\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000583543\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002180924\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000544759\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0002276552\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000527217\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0002293848\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000523403\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002359838\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000509278\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0002276423\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000493275\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0002240907\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000492944\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0002280462\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000486667\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0002442177\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000468187\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0002321862\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000457739\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0002347984\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000457638\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0002327616\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000463820\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0002302126\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000441689\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0002392176\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000440338\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002411398\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000435654\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0002342251\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000434501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002344437\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000423515\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0002376332\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000423568\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0002352303\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000436050\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0002538703\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000431489\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002272335\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000412030\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0002355143\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000409408\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002384987\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000408805\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002388946\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000408391\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002401097\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000407967\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0002405056\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000407558\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0002407128\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000407039\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0002403102\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000406677\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002403522\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000406339\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002384055\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0016700841\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002607042\n",
            "Training Results - Epoch: 2  Avg loss: 0.0144599765\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0124969325\n",
            "Training Results - Epoch: 3  Avg loss: 0.0025237436\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0009000683\n",
            "Training Results - Epoch: 4  Avg loss: 0.0024622729\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0006757853\n",
            "Training Results - Epoch: 5  Avg loss: 0.0106688163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0092193069\n",
            "Training Results - Epoch: 6  Avg loss: 0.0113228350\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0087623402\n",
            "Training Results - Epoch: 7  Avg loss: 0.0022358182\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0004764960\n",
            "Training Results - Epoch: 8  Avg loss: 0.0033973584\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0008698007\n",
            "Training Results - Epoch: 9  Avg loss: 0.0041964308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0024703868\n",
            "Training Results - Epoch: 10  Avg loss: 0.0046535211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0051427059\n",
            "Training Results - Epoch: 11  Avg loss: 0.0104188850\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0089052495\n",
            "Training Results - Epoch: 12  Avg loss: 0.0015881226\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002542211\n",
            "Training Results - Epoch: 13  Avg loss: 0.0020844326\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0005318118\n",
            "Training Results - Epoch: 14  Avg loss: 0.0041454186\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0009202456\n",
            "Training Results - Epoch: 15  Avg loss: 0.0117511057\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0109669500\n",
            "Training Results - Epoch: 16  Avg loss: 0.0043722085\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0010429269\n",
            "Training Results - Epoch: 17  Avg loss: 0.0030521708\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0006979546\n",
            "Training Results - Epoch: 18  Avg loss: 0.0019344246\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0026679491\n",
            "Training Results - Epoch: 19  Avg loss: 0.0100671595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0081840213\n",
            "Training Results - Epoch: 20  Avg loss: 0.0034497683\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0011118040\n",
            "Training Results - Epoch: 21  Avg loss: 0.0040642382\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0014207614\n",
            "Training Results - Epoch: 22  Avg loss: 0.0038821575\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0008458647\n",
            "Training Results - Epoch: 23  Avg loss: 0.0034604794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0007277716\n",
            "Training Results - Epoch: 24  Avg loss: 0.0046553936\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0017879185\n",
            "Training Results - Epoch: 25  Avg loss: 0.0060496181\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0055247844\n",
            "Training Results - Epoch: 26  Avg loss: 0.0025036816\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0009446073\n",
            "Training Results - Epoch: 27  Avg loss: 0.0020432476\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0006197440\n",
            "Training Results - Epoch: 28  Avg loss: 0.0038057786\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0007807688\n",
            "Training Results - Epoch: 29  Avg loss: 0.0014677494\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0021203412\n",
            "Training Results - Epoch: 30  Avg loss: 0.0066972952\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0038567828\n",
            "Training Results - Epoch: 31  Avg loss: 0.0038451895\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0019233827\n",
            "Training Results - Epoch: 32  Avg loss: 0.0034414987\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0007124398\n",
            "Training Results - Epoch: 33  Avg loss: 0.0035495526\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0010584038\n",
            "Training Results - Epoch: 34  Avg loss: 0.0019962928\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005333380\n",
            "Training Results - Epoch: 35  Avg loss: 0.0012007418\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0004162955\n",
            "Training Results - Epoch: 36  Avg loss: 0.0006752120\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003628592\n",
            "Training Results - Epoch: 37  Avg loss: 0.0003622188\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003011694\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002480815\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002797642\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001855455\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002672707\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001566326\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0002634496\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001433408\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002638859\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001298050\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002575339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbmY8XxTBIuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = incomes[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwljwJczL5S9",
        "colab_type": "code",
        "outputId": "8e0cdf22-862a-4423-e07a-b1d887de8a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07935540107798843"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iim2CWKFL_gV",
        "colab_type": "code",
        "outputId": "63522e02-6e18-4554-afd6-bd46cbafd468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18424671551108865"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJXQIgaBZav",
        "colab_type": "text"
      },
      "source": [
        "#### With Additional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPwMX2LsBuoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], reviews_elmo_all[2016].shape[1]//2, reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjUIN1I5P8Mv",
        "colab_type": "code",
        "outputId": "76c9ddac-33c8-47d1-9db8-064da53c387d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=512, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC-JK4z1P5ZD",
        "colab_type": "code",
        "outputId": "ae30315e-807f-4bd3-8b62-1f2b3254f781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='inc_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0209234761\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0137059486\n",
            "Training Results - Epoch: 2  Avg loss: 0.0118081027\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0050664927\n",
            "Training Results - Epoch: 3  Avg loss: 0.0222936612\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0090419727\n",
            "Training Results - Epoch: 4  Avg loss: 0.0055555309\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0012912722\n",
            "Training Results - Epoch: 5  Avg loss: 0.0079673715\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0044263674\n",
            "Training Results - Epoch: 6  Avg loss: 0.0035997835\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0009635887\n",
            "Training Results - Epoch: 7  Avg loss: 0.0023973171\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008790885\n",
            "Training Results - Epoch: 8  Avg loss: 0.0016483821\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0007659951\n",
            "Training Results - Epoch: 9  Avg loss: 0.0016910993\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0009042362\n",
            "Training Results - Epoch: 10  Avg loss: 0.0013742405\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0009612332\n",
            "Training Results - Epoch: 11  Avg loss: 0.0010536091\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0005961926\n",
            "Training Results - Epoch: 12  Avg loss: 0.0078729668\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0060396618\n",
            "Training Results - Epoch: 13  Avg loss: 0.0076263702\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0070709143\n",
            "Training Results - Epoch: 14  Avg loss: 0.0014005624\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0003204112\n",
            "Training Results - Epoch: 15  Avg loss: 0.0097919933\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0090133241\n",
            "Training Results - Epoch: 16  Avg loss: 0.0008448686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0006125423\n",
            "Training Results - Epoch: 17  Avg loss: 0.0012644361\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0012978293\n",
            "Training Results - Epoch: 18  Avg loss: 0.0010809621\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002375523\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003762410\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003087233\n",
            "Training Results - Epoch: 20  Avg loss: 0.0009573086\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0009872397\n",
            "Training Results - Epoch: 21  Avg loss: 0.0007886764\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002996379\n",
            "Training Results - Epoch: 22  Avg loss: 0.0014687753\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0011608961\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003544565\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003755605\n",
            "Training Results - Epoch: 24  Avg loss: 0.0004847013\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002735709\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002287414\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002357951\n",
            "Training Results - Epoch: 26  Avg loss: 0.0016076189\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0005357857\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003572941\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003493144\n",
            "Training Results - Epoch: 28  Avg loss: 0.0002622703\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002438932\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003392376\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002327735\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003698750\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002637104\n",
            "Training Results - Epoch: 31  Avg loss: 0.0005904781\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0005082350\n",
            "Training Results - Epoch: 32  Avg loss: 0.0003921606\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003088469\n",
            "Training Results - Epoch: 33  Avg loss: 0.0004605707\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002675257\n",
            "Training Results - Epoch: 34  Avg loss: 0.0002348902\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002384403\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003914771\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003461765\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001979115\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0002294719\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002555748\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002144987\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002022896\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002300637\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001978560\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002186293\n",
            "Training Results - Epoch: 40  Avg loss: 0.0005718805\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003076494\n",
            "Training Results - Epoch: 41  Avg loss: 0.0003806804\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003395297\n",
            "Training Results - Epoch: 42  Avg loss: 0.0090022961\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0039885530\n",
            "Training Results - Epoch: 43  Avg loss: 0.0024902905\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0010844753\n",
            "Training Results - Epoch: 44  Avg loss: 0.0060266697\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0023597043\n",
            "Training Results - Epoch: 45  Avg loss: 0.0116152811\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0051816167\n",
            "Training Results - Epoch: 46  Avg loss: 0.0525481962\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0384329599\n",
            "Training Results - Epoch: 47  Avg loss: 0.0149949582\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0104279076\n",
            "Training Results - Epoch: 48  Avg loss: 0.0190662501\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0058356013\n",
            "Training Results - Epoch: 49  Avg loss: 0.0202042015\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0040225004\n",
            "Training Results - Epoch: 50  Avg loss: 0.0047049238\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0012803573\n",
            "Training Results - Epoch: 51  Avg loss: 0.0093596344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0020376538\n",
            "Training Results - Epoch: 52  Avg loss: 0.0046426410\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0010604186\n",
            "Training Results - Epoch: 53  Avg loss: 0.0028923961\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0007205526\n",
            "Training Results - Epoch: 54  Avg loss: 0.0059846633\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0014750071\n",
            "Training Results - Epoch: 55  Avg loss: 0.0071159444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0012653235\n",
            "Training Results - Epoch: 56  Avg loss: 0.0055018083\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0020795912\n",
            "Training Results - Epoch: 57  Avg loss: 0.0033840921\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0016299571\n",
            "Training Results - Epoch: 58  Avg loss: 0.0011914511\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0005449373\n",
            "Training Results - Epoch: 59  Avg loss: 0.0007724444\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0004047563\n",
            "Training Results - Epoch: 60  Avg loss: 0.0005796363\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0003403918\n",
            "Training Results - Epoch: 61  Avg loss: 0.0004444288\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0003023699\n",
            "Training Results - Epoch: 62  Avg loss: 0.0003661908\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0002883672\n",
            "Training Results - Epoch: 63  Avg loss: 0.0003010644\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0002532526\n",
            "Training Results - Epoch: 64  Avg loss: 0.0002618664\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0002426676\n",
            "Training Results - Epoch: 65  Avg loss: 0.0002398358\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002408403\n",
            "Training Results - Epoch: 66  Avg loss: 0.0002148095\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0002216096\n",
            "Training Results - Epoch: 67  Avg loss: 0.0002040939\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002224537\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0038729064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0012777148\n",
            "Training Results - Epoch: 2  Avg loss: 0.0229069225\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0346267614\n",
            "Training Results - Epoch: 3  Avg loss: 0.0058178904\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0056231690\n",
            "Training Results - Epoch: 4  Avg loss: 0.0030466491\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0010909253\n",
            "Training Results - Epoch: 5  Avg loss: 0.0082427738\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0023305908\n",
            "Training Results - Epoch: 6  Avg loss: 0.0178509255\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0075035422\n",
            "Training Results - Epoch: 7  Avg loss: 0.0158472032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0045470217\n",
            "Training Results - Epoch: 8  Avg loss: 0.0105445064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0022246659\n",
            "Training Results - Epoch: 9  Avg loss: 0.0039132485\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0031905916\n",
            "Training Results - Epoch: 10  Avg loss: 0.0023206843\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0010838263\n",
            "Training Results - Epoch: 11  Avg loss: 0.0023979248\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0010177247\n",
            "Training Results - Epoch: 12  Avg loss: 0.0083066949\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0014815956\n",
            "Training Results - Epoch: 13  Avg loss: 0.0026281319\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0037818936\n",
            "Training Results - Epoch: 14  Avg loss: 0.0026621882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0007930952\n",
            "Training Results - Epoch: 15  Avg loss: 0.0394074076\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0516973707\n",
            "Training Results - Epoch: 16  Avg loss: 0.0027989776\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0027665930\n",
            "Training Results - Epoch: 17  Avg loss: 0.0007540434\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003533840\n",
            "Training Results - Epoch: 18  Avg loss: 0.0052969720\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0035539985\n",
            "Training Results - Epoch: 19  Avg loss: 0.0027602705\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0027453763\n",
            "Training Results - Epoch: 20  Avg loss: 0.0066798491\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0053626495\n",
            "Training Results - Epoch: 21  Avg loss: 0.0031332454\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0009457357\n",
            "Training Results - Epoch: 22  Avg loss: 0.0063796129\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0010165460\n",
            "Training Results - Epoch: 23  Avg loss: 0.0025797371\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0015682562\n",
            "Training Results - Epoch: 24  Avg loss: 0.0098357127\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0060416971\n",
            "Training Results - Epoch: 25  Avg loss: 0.0010043795\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004552516\n",
            "Training Results - Epoch: 26  Avg loss: 0.0015178256\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0008334461\n",
            "Training Results - Epoch: 27  Avg loss: 0.0016487649\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0010943685\n",
            "Training Results - Epoch: 28  Avg loss: 0.0006669607\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0008082921\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002661461\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003262794\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003279514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0004524963\n",
            "Training Results - Epoch: 31  Avg loss: 0.0002108049\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003487392\n",
            "Training Results - Epoch: 32  Avg loss: 0.0016971686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0019066113\n",
            "Training Results - Epoch: 33  Avg loss: 0.0007612751\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0009475802\n",
            "Training Results - Epoch: 34  Avg loss: 0.0006453073\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0006277307\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003280915\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003416151\n",
            "Training Results - Epoch: 36  Avg loss: 0.0003148673\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003728634\n",
            "Training Results - Epoch: 37  Avg loss: 0.0003565701\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003745997\n",
            "Training Results - Epoch: 38  Avg loss: 0.0014369330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0016561772\n",
            "Training Results - Epoch: 39  Avg loss: 0.0007395268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0006609031\n",
            "Training Results - Epoch: 40  Avg loss: 0.0002121320\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003315237\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001745075\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003214807\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001716635\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003411186\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001870711\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002892160\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002543216\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003330368\n",
            "Training Results - Epoch: 45  Avg loss: 0.0002691156\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003989186\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001495828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0003068169\n",
            "Training Results - Epoch: 47  Avg loss: 0.0001504166\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0003017406\n",
            "Training Results - Epoch: 48  Avg loss: 0.0001423550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0003193731\n",
            "Training Results - Epoch: 49  Avg loss: 0.0001608907\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002985776\n",
            "Training Results - Epoch: 50  Avg loss: 0.0003652330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0003690072\n",
            "Training Results - Epoch: 51  Avg loss: 0.0005442092\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0006195181\n",
            "Training Results - Epoch: 52  Avg loss: 0.0003272453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0003820556\n",
            "Training Results - Epoch: 53  Avg loss: 0.0004523339\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0004208234\n",
            "Training Results - Epoch: 54  Avg loss: 0.0007650150\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0004854107\n",
            "Training Results - Epoch: 55  Avg loss: 0.0011536117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0004103375\n",
            "Training Results - Epoch: 56  Avg loss: 0.0013891462\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005507194\n",
            "Training Results - Epoch: 57  Avg loss: 0.0103998809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0070389733\n",
            "Training Results - Epoch: 58  Avg loss: 0.0026323652\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0020075709\n",
            "Training Results - Epoch: 59  Avg loss: 0.0278745411\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0159392082\n",
            "Training Results - Epoch: 60  Avg loss: 0.0372452493\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0480063804\n",
            "Training Results - Epoch: 61  Avg loss: 0.0284605774\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0195518723\n",
            "Training Results - Epoch: 62  Avg loss: 0.0158445250\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0031058119\n",
            "Training Results - Epoch: 63  Avg loss: 0.0124074569\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0025235065\n",
            "Training Results - Epoch: 64  Avg loss: 0.0076041246\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0085056117\n",
            "Training Results - Epoch: 65  Avg loss: 0.0017666549\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0012855284\n",
            "Training Results - Epoch: 66  Avg loss: 0.0009898073\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0007255966\n",
            "Training Results - Epoch: 67  Avg loss: 0.0007333428\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0006269001\n",
            "Training Results - Epoch: 68  Avg loss: 0.0005426778\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0006130984\n",
            "Training Results - Epoch: 69  Avg loss: 0.0004220344\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0004934772\n",
            "Training Results - Epoch: 70  Avg loss: 0.0003368316\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0004778039\n",
            "Training Results - Epoch: 71  Avg loss: 0.0002766353\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0004296267\n",
            "Training Results - Epoch: 72  Avg loss: 0.0002409093\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0004097399\n",
            "Training Results - Epoch: 73  Avg loss: 0.0002139220\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0004064392\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0089497009\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0036850642\n",
            "Training Results - Epoch: 2  Avg loss: 0.0060317655\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0045329693\n",
            "Training Results - Epoch: 3  Avg loss: 0.0062740064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0022490372\n",
            "Training Results - Epoch: 4  Avg loss: 0.0051331523\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0014068924\n",
            "Training Results - Epoch: 5  Avg loss: 0.0046134076\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0023371136\n",
            "Training Results - Epoch: 6  Avg loss: 0.0166671898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0093681441\n",
            "Training Results - Epoch: 7  Avg loss: 0.0026040381\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0007105530\n",
            "Training Results - Epoch: 8  Avg loss: 0.0038552185\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0012596538\n",
            "Training Results - Epoch: 9  Avg loss: 0.0186531285\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0147233224\n",
            "Training Results - Epoch: 10  Avg loss: 0.0059663665\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0013919593\n",
            "Training Results - Epoch: 11  Avg loss: 0.0075280385\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0067116433\n",
            "Training Results - Epoch: 12  Avg loss: 0.0028430127\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0012243952\n",
            "Training Results - Epoch: 13  Avg loss: 0.0013943097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0007055369\n",
            "Training Results - Epoch: 14  Avg loss: 0.0035520103\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0016306185\n",
            "Training Results - Epoch: 15  Avg loss: 0.0027901019\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0016005172\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004190780\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005760932\n",
            "Training Results - Epoch: 17  Avg loss: 0.0008295520\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0007143410\n",
            "Training Results - Epoch: 18  Avg loss: 0.0011961972\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0005647859\n",
            "Training Results - Epoch: 19  Avg loss: 0.0025606984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0026361033\n",
            "Training Results - Epoch: 20  Avg loss: 0.0053685981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0034145841\n",
            "Training Results - Epoch: 21  Avg loss: 0.0010999160\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0009710301\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003699659\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005027756\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003182306\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004663294\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003965659\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003608500\n",
            "Training Results - Epoch: 25  Avg loss: 0.0004454648\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0006853756\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002404861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0003595489\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001474510\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003075489\n",
            "Training Results - Epoch: 28  Avg loss: 0.0005153328\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0004927386\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002273229\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003210493\n",
            "Training Results - Epoch: 30  Avg loss: 0.0002173277\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003250775\n",
            "Training Results - Epoch: 31  Avg loss: 0.0003515880\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0004325345\n",
            "Training Results - Epoch: 32  Avg loss: 0.0003081302\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003620909\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001847137\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003217619\n",
            "Training Results - Epoch: 34  Avg loss: 0.0004093982\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005036369\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001773288\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003676005\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001651176\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003445578\n",
            "Training Results - Epoch: 37  Avg loss: 0.0001927363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003698697\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002053760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003676610\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001620566\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003336309\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001584265\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003592438\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001561553\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003638292\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001987560\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003812848\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001949492\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0004011761\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002050714\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003619384\n",
            "Training Results - Epoch: 45  Avg loss: 0.0003337731\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0004601343\n",
            "Training Results - Epoch: 46  Avg loss: 0.0007984981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0007497369\n",
            "Training Results - Epoch: 47  Avg loss: 0.0017413884\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0007266005\n",
            "Training Results - Epoch: 48  Avg loss: 0.0473043700\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0312446314\n",
            "Training Results - Epoch: 49  Avg loss: 0.0016566562\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0015785092\n",
            "Training Results - Epoch: 50  Avg loss: 0.0007061341\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0009872793\n",
            "Training Results - Epoch: 51  Avg loss: 0.0004989156\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0008016587\n",
            "Training Results - Epoch: 52  Avg loss: 0.0003448172\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0007211621\n",
            "Training Results - Epoch: 53  Avg loss: 0.0002962018\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0006298768\n",
            "Training Results - Epoch: 54  Avg loss: 0.0002793758\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0006854306\n",
            "Training Results - Epoch: 55  Avg loss: 0.0002645570\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0006417313\n",
            "Training Results - Epoch: 56  Avg loss: 0.0002405682\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005742923\n",
            "Training Results - Epoch: 57  Avg loss: 0.0003102604\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0004974108\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0041864310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0032350246\n",
            "Training Results - Epoch: 2  Avg loss: 0.0014447955\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0012297388\n",
            "Training Results - Epoch: 3  Avg loss: 0.0005883858\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0007902917\n",
            "Training Results - Epoch: 4  Avg loss: 0.0011654814\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0005776650\n",
            "Training Results - Epoch: 5  Avg loss: 0.0007793346\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0005785700\n",
            "Training Results - Epoch: 6  Avg loss: 0.0006311820\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0006279364\n",
            "Training Results - Epoch: 7  Avg loss: 0.0005563295\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0005317549\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006030010\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0004200197\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006547189\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0006951400\n",
            "Training Results - Epoch: 10  Avg loss: 0.0003365365\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0004112100\n",
            "Training Results - Epoch: 11  Avg loss: 0.0003088294\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0005230345\n",
            "Training Results - Epoch: 12  Avg loss: 0.0002783316\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0004403394\n",
            "Training Results - Epoch: 13  Avg loss: 0.0002271171\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004392457\n",
            "Training Results - Epoch: 14  Avg loss: 0.0003266450\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0006880335\n",
            "Training Results - Epoch: 15  Avg loss: 0.0002032064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003389726\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001694008\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005682227\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001714486\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003320728\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001636556\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004404894\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001603541\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0004158700\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001725559\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005415348\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001547136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0004658510\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001933522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0004038450\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001671308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003585448\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001843023\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0004699639\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002276218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0005319022\n",
            "Training Results - Epoch: 26  Avg loss: 0.0004216349\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0009657347\n",
            "Training Results - Epoch: 27  Avg loss: 0.0004870354\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0011018034\n",
            "Training Results - Epoch: 28  Avg loss: 0.0065315928\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0034013633\n",
            "Training Results - Epoch: 29  Avg loss: 0.0251763065\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0182716622\n",
            "Training Results - Epoch: 30  Avg loss: 0.0153312046\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0059843473\n",
            "Training Results - Epoch: 31  Avg loss: 0.0198425814\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0135966580\n",
            "Training Results - Epoch: 32  Avg loss: 0.0076906974\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0041457324\n",
            "Training Results - Epoch: 33  Avg loss: 0.0063398786\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0025473875\n",
            "Training Results - Epoch: 34  Avg loss: 0.0019710988\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0022784181\n",
            "Training Results - Epoch: 35  Avg loss: 0.0007683499\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0006990262\n",
            "Training Results - Epoch: 36  Avg loss: 0.0003332808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0005041160\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002024002\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003203892\n",
            "Training Results - Epoch: 38  Avg loss: 0.0001370140\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003875835\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001253532\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003536274\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001159871\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003984520\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001110735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003635817\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001115984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0004826488\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001140851\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003663225\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001334193\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0004335232\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001539395\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0004248577\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001607268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0004723642\n",
            "Training Results - Epoch: 47  Avg loss: 0.0002794304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0006329498\n",
            "Training Results - Epoch: 48  Avg loss: 0.0003918004\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0005479814\n",
            "Training Results - Epoch: 49  Avg loss: 0.0007709601\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0013378550\n",
            "Training Results - Epoch: 50  Avg loss: 0.0007687096\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0006135324\n",
            "Training Results - Epoch: 51  Avg loss: 0.0009374586\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0026619852\n",
            "Training Results - Epoch: 52  Avg loss: 0.0006375069\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0005800714\n",
            "Training Results - Epoch: 53  Avg loss: 0.0004670157\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0004723161\n",
            "Training Results - Epoch: 54  Avg loss: 0.0005899084\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0010296337\n",
            "Training Results - Epoch: 55  Avg loss: 0.0009687873\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0007938248\n",
            "Training Results - Epoch: 56  Avg loss: 0.0007169211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005943374\n",
            "Training Results - Epoch: 57  Avg loss: 0.0005770627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0006592409\n",
            "Training Results - Epoch: 58  Avg loss: 0.0003898047\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0011863857\n",
            "Training Results - Epoch: 59  Avg loss: 0.0002037393\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0007257291\n",
            "Training Results - Epoch: 60  Avg loss: 0.0001520207\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0005672982\n",
            "Training Results - Epoch: 61  Avg loss: 0.0001307462\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0004881981\n",
            "Training Results - Epoch: 62  Avg loss: 0.0001190812\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0004659850\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001125389\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0004632181\n",
            "Training Results - Epoch: 64  Avg loss: 0.0001072343\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0004577145\n",
            "Training Results - Epoch: 65  Avg loss: 0.0001031916\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0004551430\n",
            "Training Results - Epoch: 66  Avg loss: 0.0001003077\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0004484669\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000977119\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0004450960\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0026753869\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0011764320\n",
            "Training Results - Epoch: 2  Avg loss: 0.0061566276\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0024067458\n",
            "Training Results - Epoch: 3  Avg loss: 0.0048249134\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0049548519\n",
            "Training Results - Epoch: 4  Avg loss: 0.0029682975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0023439784\n",
            "Training Results - Epoch: 5  Avg loss: 0.0054778639\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0024264946\n",
            "Training Results - Epoch: 6  Avg loss: 0.0053288657\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0043145419\n",
            "Training Results - Epoch: 7  Avg loss: 0.0005755186\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008912145\n",
            "Training Results - Epoch: 8  Avg loss: 0.0011198694\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0004875630\n",
            "Training Results - Epoch: 9  Avg loss: 0.0013917609\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0007025985\n",
            "Training Results - Epoch: 10  Avg loss: 0.0016523514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0005520036\n",
            "Training Results - Epoch: 11  Avg loss: 0.0017206057\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0005755205\n",
            "Training Results - Epoch: 12  Avg loss: 0.0012113883\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0008178788\n",
            "Training Results - Epoch: 13  Avg loss: 0.0010724876\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0006108455\n",
            "Training Results - Epoch: 14  Avg loss: 0.0012115590\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0016225483\n",
            "Training Results - Epoch: 15  Avg loss: 0.0022642234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0016307125\n",
            "Training Results - Epoch: 16  Avg loss: 0.0018101984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0006910805\n",
            "Training Results - Epoch: 17  Avg loss: 0.0055867898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0057834338\n",
            "Training Results - Epoch: 18  Avg loss: 0.0021253960\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0015945504\n",
            "Training Results - Epoch: 19  Avg loss: 0.0033427416\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0015382619\n",
            "Training Results - Epoch: 20  Avg loss: 0.0019276809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0022985425\n",
            "Training Results - Epoch: 21  Avg loss: 0.0021504595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0011855787\n",
            "Training Results - Epoch: 22  Avg loss: 0.0045612264\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0048575639\n",
            "Training Results - Epoch: 23  Avg loss: 0.0012709431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0013663962\n",
            "Training Results - Epoch: 24  Avg loss: 0.0019475034\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0007923730\n",
            "Training Results - Epoch: 25  Avg loss: 0.0022636074\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0011877449\n",
            "Training Results - Epoch: 26  Avg loss: 0.0028844945\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0025868662\n",
            "Training Results - Epoch: 27  Avg loss: 0.0013930578\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0009175495\n",
            "Training Results - Epoch: 28  Avg loss: 0.0049076453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0016283409\n",
            "Training Results - Epoch: 29  Avg loss: 0.0034580017\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0026146425\n",
            "Training Results - Epoch: 30  Avg loss: 0.0007873175\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0005012187\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001768561\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003756880\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001479641\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003648741\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001412960\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003613803\n",
            "Training Results - Epoch: 34  Avg loss: 0.0001377016\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003591035\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001339549\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003598687\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001315893\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003592067\n",
            "Training Results - Epoch: 37  Avg loss: 0.0001293293\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003596986\n",
            "Training Results - Epoch: 38  Avg loss: 0.0001273779\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003606105\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001250185\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003601069\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001229741\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003602430\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001214451\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003607128\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001195315\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003619566\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001184004\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003614749\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001166890\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003640281\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001152209\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003636170\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001142142\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0003660572\n",
            "Training Results - Epoch: 47  Avg loss: 0.0001126484\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0003684535\n",
            "Training Results - Epoch: 48  Avg loss: 0.0001116583\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0003672473\n",
            "Training Results - Epoch: 49  Avg loss: 0.0001107850\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0003706315\n",
            "Training Results - Epoch: 50  Avg loss: 0.0001095273\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0003737232\n",
            "Training Results - Epoch: 51  Avg loss: 0.0001089698\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0003757131\n",
            "Training Results - Epoch: 52  Avg loss: 0.0001076364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0003743281\n",
            "Training Results - Epoch: 53  Avg loss: 0.0001067656\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0003780510\n",
            "Training Results - Epoch: 54  Avg loss: 0.0001053103\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0003810436\n",
            "Training Results - Epoch: 55  Avg loss: 0.0001060888\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0003845089\n",
            "Training Results - Epoch: 56  Avg loss: 0.0001052879\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0003845151\n",
            "Training Results - Epoch: 57  Avg loss: 0.0001046002\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0003846731\n",
            "Training Results - Epoch: 58  Avg loss: 0.0001042835\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0003849316\n",
            "Training Results - Epoch: 59  Avg loss: 0.0001040707\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0003852878\n",
            "Training Results - Epoch: 60  Avg loss: 0.0001038968\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0003854908\n",
            "Training Results - Epoch: 61  Avg loss: 0.0001037616\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0003858282\n",
            "Training Results - Epoch: 62  Avg loss: 0.0001036275\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0003862140\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001035038\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0003868515\n",
            "Training Results - Epoch: 64  Avg loss: 0.0001034029\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0003871988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5RM5xCQBeJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = incomes[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b7d74c6-c411-441d-cf7c-b648c127125e",
        "id": "PJO-Vs__BhP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0858662570687752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKVsosH0Ok-U",
        "colab_type": "code",
        "outputId": "fdad3275-e1e5-4e13-979c-366030a745db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.131040148882963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Jwz6sHN4jD",
        "colab_type": "text"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzmVh3vN6Ji",
        "colab_type": "text"
      },
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th rowspan=2>Model</th>\n",
        "            <th rowspan=2> \"No change\" </th>\n",
        "            <th colspan=2>TF-IDF Autoencoder</th>\n",
        "            <th colspan=2>ELMO</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>NLR2</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>Training MAE</td>\n",
        "            <td>14.76%</td>\n",
        "            <td>10.07%</td>\n",
        "            <td>9.89%</td>\n",
        "            <td>9.78%</td>\n",
        "            <td>7.94%</td>\n",
        "            <td>8.59%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Testing MAE</td>\n",
        "            <td>14.10%</td>\n",
        "            <td>10.32%</td>\n",
        "            <td>12.90%</td>\n",
        "            <td>10.63%</td>\n",
        "            <td>18.42%</td>\n",
        "            <td>13.10%</td>\n",
        "        </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4pqGXLIdmgA",
        "colab_type": "text"
      },
      "source": [
        "## Sensitivity Analysis\n",
        "\n",
        "Senesitivity Analysis is done on the best performing model, TF-IDF Autoencoder with Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n93Vnj4zdwrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_dim_reviews(reviews, orig_delta_embedding, dim):\n",
        "    orig_decoded = lr.predict(orig_delta_embedding)\n",
        "    diff = np.zeros(orig_decoded.shape)\n",
        "  \n",
        "    r_2011 = reviews[2011].data.clone()\n",
        "    r_2016 = reviews[2016].data.clone()\n",
        "\n",
        "    r_2011[:, dim] = torch.zeros(r_2011.shape[0])\n",
        "    r_2016[:, dim] = torch.zeros(r_2016.shape[0])\n",
        "\n",
        "    delta_embedding = encoder(r_2016) - encoder(r_2011)\n",
        "\n",
        "    decoded = lr.predict(delta_embedding.detach().cpu().numpy())\n",
        "    diff = orig_decoded - decoded\n",
        "\n",
        "    diff = diff.mean(axis=0)\n",
        "    \n",
        "    return diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcfvLHM3eHBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importance_reviews = np.zeros((reviews[2011].data.shape[1], delta_census.shape[1]))\n",
        "for i in range(reviews[2011].data.shape[1]):\n",
        "  feature_importance_reviews[i] = evaluate_dim_reviews(reviews, delta_embedding, i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNLcXN7ebDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort by absolute mean\n",
        "sort_importance = np.argsort(np.abs(feature_importance_reviews).mean(axis=1))[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzWh_8AVeeBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(DRIVE_PATH.joinpath('models/words.out'), 'r') as f:\n",
        "  words = f.read().split('\\n')\n",
        "\n",
        "words = words[:-1]\n",
        "\n",
        "df_importance = pd.DataFrame(feature_importance_reviews, columns=incomes[2011].columns, index=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrBDkO32eivt",
        "colab_type": "code",
        "outputId": "e4b050fa-eac6-4f19-fc57-cd8f697d812e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "df_importance.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['  Under $10,000', '  $10,000 to $19,999', '  $20,000 to $29,999',\n",
              "       '  $30,000 to $39,999', '  $40,000 to $49,999', '  $50,000 to $59,999',\n",
              "       '  $60,000 to $79,999', '  $80,000 to $99,999', '  $100,000 and over'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njcZhNzde1ld",
        "colab_type": "code",
        "outputId": "7e7cbd85-352d-4cbd-af4f-16b21b3bbf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Low income\n",
        "df_importance['  Under $10,000'][df_importance['  Under $10,000'].abs().sort_values(ascending=False)[3:13].index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "really good         0.000025\n",
              "violating           0.000024\n",
              "customer service   -0.000020\n",
              "patio              -0.000014\n",
              "amazing            -0.000014\n",
              "sales               0.000014\n",
              "customer           -0.000012\n",
              "deep fried         -0.000012\n",
              "fish               -0.000012\n",
              "sashimi             0.000011\n",
              "Name:   Under $10,000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWHIKm1ve4k7",
        "colab_type": "code",
        "outputId": "e8d275c2-f4b5-4771-f314-84c561a69ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Mid-level\n",
        "df_importance['  $50,000 to $59,999'][df_importance['  $50,000 to $59,999'].abs().sort_values(ascending=False)[3:13].index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "violating terms     0.000036\n",
              "burger             -0.000022\n",
              "chips              -0.000016\n",
              "customer service   -0.000015\n",
              "jerk                0.000014\n",
              "service            -0.000013\n",
              "coffee              0.000013\n",
              "ice cream          -0.000013\n",
              "pizza              -0.000012\n",
              "school              0.000011\n",
              "Name:   $50,000 to $59,999, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMFNW6pJe-Ou",
        "colab_type": "code",
        "outputId": "b3c903b0-0c1c-45e8-bbe3-8de5256dddf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# High-income\n",
        "df_importance['  $100,000 and over'][df_importance['  $100,000 and over'].abs().sort_values(ascending=False)[:10].index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "chicken             0.000032\n",
              "review removed      0.000020\n",
              "highly recommend    0.000014\n",
              "store              -0.000011\n",
              "sashimi            -0.000011\n",
              "salmon             -0.000010\n",
              "school              0.000010\n",
              "really good        -0.000009\n",
              "burger             -0.000009\n",
              "class               0.000008\n",
              "Name:   $100,000 and over, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}