{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CensusPrediction_Education.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5QEoXb0vhch",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmm2_PLSXXms",
        "colab_type": "code",
        "outputId": "3ab7c0f9-824c-4797-9c8c-48ff3e2e73f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UhiP9qfJOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy import sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xsyi3AX56i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ignite.handlers import ModelCheckpoint, EarlyStopping, TerminateOnNan\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
        "from ignite.contrib.handlers import ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwPy5Lh3CH-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import string\n",
        "import random\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itB2xTKXZm3o",
        "colab_type": "code",
        "outputId": "37b977b6-7a05-4996-8322-1fc1527cf64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "import locale\n",
        "from locale import atof, atoi\n",
        "locale.setlocale(locale.LC_NUMERIC, '')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF-8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBzBYsqc1OAw",
        "colab_type": "code",
        "outputId": "ca651f7a-c3e7-4769-f0b3-1452f1c33d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmD0YDfwfafe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_idf = True\n",
        "max_features = 2**10\n",
        "pca_components = 2**4\n",
        "DRIVE_PATH = Path('/content/drive/My Drive/Thesis2019/')\n",
        "NEIGHBOURHOOD_PROFILES_PATH = Path('neighbourhood-profiles-2016-csv.csv')\n",
        "NEIGHBOURHOOD_PROFILES_PATH_OLD = Path('neighbourhood-data-2001-2011.xlsx')\n",
        "DATA_DIR = DRIVE_PATH.joinpath('data_stripped')\n",
        "use_cuda = True\n",
        "years = [2011, 2016]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KxXDNKRxRpT",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHv4QW7DYBKP",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgKDOkoxTQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewsVector(Data.Dataset):\n",
        "    \"\"\"Reviews Vector dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          self.data = self.data.cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class CensusVector(Data.Dataset):\n",
        "  def __init__(self, data, reviews_embedding):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        self.reviews_embedding = torch.tensor(reviews_embedding).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            self.data = self.data.cuda()\n",
        "            self.reviews_embedding = self.reviews_embedding.cuda()\n",
        "            \n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return {\"data\": self.data[idx],\n",
        "              \"reviews_embedding\": self.reviews_embedding[idx]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0yvBkQovmDE",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdiWF8NSwfqx",
        "colab_type": "text"
      },
      "source": [
        "## Selecting and Matching Attributes for 2011 and 2016 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiwEZ3YDYGGC",
        "colab_type": "text"
      },
      "source": [
        "Match census categories from different years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV574nlTv42q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbh_profiles = pd.read_csv(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH))\n",
        "nbh_profiles_2011 = pd.read_excel(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH_OLD), sheet_name='2011')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7N_jBiOZonh",
        "colab_type": "code",
        "outputId": "74a77c3d-b7ca-4210-d33a-11eedd7fc1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "list(nbh_profiles_2011[nbh_profiles_2011['Topic']=='Highest certificate, diploma or degree']['Attribute'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Total population aged 15 years and over by highest certificate, diploma or degree',\n",
              " '  No certificate, diploma or degree',\n",
              " '  High school diploma or equivalent',\n",
              " '  Postsecondary certificate, diploma or degree',\n",
              " '    Apprenticeship or trades certificate or diploma',\n",
              " '    College, CEGEP or other non-university certificate or diploma',\n",
              " '    University certificate or diploma below bachelor level',\n",
              " '    University certificate, diploma or degree at bachelor level or above',\n",
              " \"      Bachelor's degree\",\n",
              " '      University certificate, diploma or degree above bachelor level',\n",
              " 'Total population aged 25 to 64 years by highest certificate, diploma or degree',\n",
              " '  No certificate, diploma or degree',\n",
              " '  High school diploma or equivalent',\n",
              " '  Postsecondary certificate, diploma or degree',\n",
              " '    Apprenticeship or trades certificate or diploma',\n",
              " '    College, CEGEP or other non-university certificate or diploma',\n",
              " '    University certificate or diploma below bachelor level',\n",
              " '    University certificate, diploma or degree at bachelor level or above',\n",
              " \"      Bachelor's degree\",\n",
              " '      University certificate, diploma or degree above bachelor level']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HOILGeIcQp_",
        "colab_type": "code",
        "outputId": "8727f3d7-a2fd-4c00-b8e3-77ea070fd402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "list(nbh_profiles[nbh_profiles['Topic']=='Highest certificate, diploma or degree']['Characteristic'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"      Master's degree\",\n",
              " '  Postsecondary certificate, diploma or degree',\n",
              " '    Apprenticeship or trades certificate or diploma',\n",
              " 'Total - Highest certificate, diploma or degree for the population aged 15 years and over in private households - 25% sample data',\n",
              " '  No certificate, diploma or degree',\n",
              " '  Secondary (high) school diploma or equivalency certificate',\n",
              " '      Trades certificate or diploma other than Certificate of Apprenticeship or Certificate of Qualification',\n",
              " '      Certificate of Apprenticeship or Certificate of Qualification',\n",
              " '    College, CEGEP or other non-university certificate or diploma',\n",
              " '    University certificate or diploma below bachelor level',\n",
              " '    University certificate, diploma or degree at bachelor level or above',\n",
              " \"      Bachelor's degree\",\n",
              " '      University certificate or diploma above bachelor level',\n",
              " '      Degree in medicine, dentistry, veterinary medicine or optometry',\n",
              " '      Earned doctorate',\n",
              " 'Total - Highest certificate, diploma or degree for the population aged 25 to 64 years in private households - 25% sample data',\n",
              " '  No certificate, diploma or degree',\n",
              " '  Secondary (high) school diploma or equivalency certificate',\n",
              " '  Postsecondary certificate, diploma or degree',\n",
              " '    Apprenticeship or trades certificate or diploma',\n",
              " '      Trades certificate or diploma other than Certificate of Apprenticeship or Certificate of Qualification',\n",
              " \"      Master's degree\",\n",
              " '      Certificate of Apprenticeship or Certificate of Qualification',\n",
              " '    College, CEGEP or other non-university certificate or diploma',\n",
              " '    University certificate or diploma below bachelor level',\n",
              " '    University certificate, diploma or degree at bachelor level or above',\n",
              " \"      Bachelor's degree\",\n",
              " '      University certificate or diploma above bachelor level',\n",
              " '      Degree in medicine, dentistry, veterinary medicine or optometry',\n",
              " '      Earned doctorate']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadRLgh2wKM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_2011 = nbh_profiles_2011[(nbh_profiles_2011['Topic']=='Highest certificate, diploma or degree')].drop(columns=['Category', 'Topic', 'City of Toronto'])\n",
        "education_2016 = nbh_profiles[(nbh_profiles['Topic']=='Highest certificate, diploma or degree')].drop(columns=['_id', 'Category', 'Topic', 'Data Source', 'City of Toronto'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE2nDlv9wQ1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_2011 = education_2011[education_2011['Attribute'] == 'Total population aged 25 to 64 years by highest certificate, diploma or degree'].index\n",
        "education_2011 = education_2011.loc[idx_2011[0]+1:]\n",
        "education_2011.set_index('Attribute', inplace=True)\n",
        "education_2011_clean = education_2011.T[['  No certificate, diploma or degree', \n",
        "                                       '  High school diploma or equivalent', \n",
        "                                       '    Apprenticeship or trades certificate or diploma', \n",
        "                                       '    College, CEGEP or other non-university certificate or diploma', \n",
        "                                       '    University certificate or diploma below bachelor level',\n",
        "                                       '    University certificate, diploma or degree at bachelor level or above']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZ6_7VfwTbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_2016 = education_2016[education_2016['Characteristic'] == 'Total - Highest certificate, diploma or degree for the population aged 25 to 64 years in private households - 25% sample data'].index\n",
        "education_2016 = education_2016.loc[idx_2016[0]+1:]\n",
        "education_2016.set_index('Characteristic', inplace=True)\n",
        "education_2016_clean = education_2016.T[['  No certificate, diploma or degree', \n",
        "                                       '  Secondary (high) school diploma or equivalency certificate', \n",
        "                                       '    Apprenticeship or trades certificate or diploma', \n",
        "                                       '    College, CEGEP or other non-university certificate or diploma', \n",
        "                                       '    University certificate or diploma below bachelor level',\n",
        "                                       '    University certificate, diploma or degree at bachelor level or above']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ77wNPFwcbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_2011_clean.sort_index(inplace=True)\n",
        "education_2016_clean.sort_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtYHcD37wdn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_2016_clean = education_2016_clean.applymap(atoi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZYuOztQw5Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned = [education_2011_clean, education_2016_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmSvGTznxLVP",
        "colab_type": "text"
      },
      "source": [
        "## Build Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48J_0qfUwsvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build Census datasets\n",
        "educations = {}\n",
        "for education in cleaned:\n",
        "  educations[year] = education.div(education.sum(axis=1), axis=0) # Normalize to census proportions\n",
        "  education.to_csv(DRIVE_PATH.joinpath('education_'+str(year)+'.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_q_9zDIxom",
        "colab_type": "text"
      },
      "source": [
        "## Load Built Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAwtOk-zIxCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "educations = {}\n",
        "\n",
        "for year in years:\n",
        "  educations[year] = pd.read_csv(DRIVE_PATH.joinpath('education_{}.csv'.format(year)))\n",
        "  educations[year].set_index('Unnamed: 0', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtT3yMRfxfY5",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Census Change\n",
        "\n",
        "Census Change prediction is evaluated with Mean Total Absolute Error:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBZZj91qKMy",
        "colab_type": "text"
      },
      "source": [
        "$Mean Total Absolute Error = $\n",
        "\n",
        "$\\frac{1}{\\#Neighbourhoods}\\sum_{Neighbourhoods} \\sum_{categories} |Actual Proportion - Predicted Proportion|$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$categories \\in \\{\\text{No degree, High school diploma, Apprenticeship certificate, College diploma, University certificate below bachelor level, University degree above bachelor level}\\}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34dueeBjxz6o",
        "colab_type": "text"
      },
      "source": [
        "### Train-test split\n",
        "\n",
        "Split by neighbourhoods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTuAGFwx4EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_neighbourhoods, test_neighbourhoods = train_test_split(range(0, 140), test_size=0.15)\n",
        "\n",
        "all_trains, all_vals = [], [] #Do random splits to cross validate\n",
        "folds = 5\n",
        "\n",
        "for i in range(folds):\n",
        "  train_neighbourhoods, val_neighbourhoods = train_test_split(train_val_neighbourhoods, test_size=0.30)\n",
        "  all_trains.append(train_neighbourhoods)\n",
        "  all_vals.append(val_neighbourhoods)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9kM4UO-8pA",
        "colab_type": "text"
      },
      "source": [
        "### Baseline-Predicting no change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrDG6hs-_Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_data = educations[2011].values\n",
        "actual_data = educations[2016].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuyEnG-Q8yGn",
        "colab_type": "code",
        "outputId": "dce9f3af-df5a-4271-ca43-e3818c16f044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(dummy_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11922240025291206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4a7r_zD82P2",
        "colab_type": "code",
        "outputId": "0456ff59-20a0-4b15-ee6b-5dbc62ac5e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(dummy_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12119099541854524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5675IoiJxhit",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlYfoxBu3S_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Encoder, self).__init__()\n",
        "                \n",
        "        layers_en = OrderedDict()       \n",
        "        for i in range(len(sizes)-1):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_en[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            if i==0:\n",
        "                nn.init.xavier_uniform_(layers_en[layer_name].weight)\n",
        "            layers_en[act_name] = nn.Tanh() #-1 to 1\n",
        "        \n",
        "        self.encoder = nn.Sequential(layers_en)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x) \n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        sizes = sizes[::-1]\n",
        "        \n",
        "        layers_de = OrderedDict()\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        layers_de['sigmoid'] = nn.Sigmoid() #0 to 1\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQbaL7TxjKb",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model and built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEZ_P7n_-3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_all = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_all[year] = pd.read_csv(DRIVE_PATH.joinpath('neighbourhood_reviews_{}.csv'.format(year)))\n",
        "  reviews_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "reviews = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews[year] = ReviewsVector(reviews_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhvmqz3xrNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X44OFzttxs10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_r = [max_features, 2**10, 2**8, pca_components]\n",
        "encoder = Encoder(sizes_r)\n",
        "decoder = Decoder(sizes_r)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGf84g_Oh34S",
        "colab_type": "code",
        "outputId": "45bb3655-f5b5-43b5-9be5-d3e5f404cfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "encoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_encoder2_500.pth'))\n",
        "encoder.eval()\n",
        "encoder.to(torch.device(\"cuda\"))\n",
        "\n",
        "decoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_decoder2_500.pth'))\n",
        "decoder.eval()\n",
        "decoder.to(torch.device(\"cuda\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=16, out_features=256, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
              "    (activation2): Tanh()\n",
              "    (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71OoLpU43hWF",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuCjk_J3lsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import RidgeCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7z_0fK3vee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = RidgeCV(cv=5)\n",
        "delta_census = educations[2016].values - educations[2011].values\n",
        "delta_embedding = (encoder(reviews[2016].data) - encoder(reviews[2011].data)).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApOZxNV3xtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(delta_embedding[train_val_neighbourhoods], delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6IgNOM03y5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lr.predict(delta_embedding[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC53L79OzAuS",
        "colab_type": "code",
        "outputId": "6bf7916a-7ea0-4180-d597-cd6ac5959022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07941866311359919"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM4QZ_tazKTy",
        "colab_type": "code",
        "outputId": "467e9aab-af37-4d93-a477-6c25ddf59223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((lr.predict(delta_embedding[test_neighbourhoods])-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10068076677944726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5ldoCryItc",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression (Census Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLhG_VEh3cVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder_C(nn.Module):\n",
        "    def __init__(self, sizes, softmax=False):\n",
        "        super(Decoder_C, self).__init__()\n",
        "        sizes = sizes[::-1]\n",
        "        layers_de = OrderedDict()\n",
        "\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        # layers_de['softmax'] = nn.Softmax(dim=1) # row sum to 1\n",
        "        layers_de['tanh'] = nn.Tanh()\n",
        "        if softmax:\n",
        "          layers_de['softmax'] = nn.Softmax(dim=1)\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBXycsVr3Z6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion_c(data, decoded):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    loss = mse_loss(data, decoded)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKQe1WQ_-V0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_decoder(decoder, dataset, train_indices, test_indices, name='edu_2'):\n",
        "  optimizer_de = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "  scheduler_de = optim.lr_scheduler.ReduceLROnPlateau(optimizer_de, 'min', patience=20, min_lr=min_lr, factor=0.1)\n",
        "\n",
        "  def process_function(engine, batch):\n",
        "    decoder.train()\n",
        "    optimizer_de.zero_grad()\n",
        "    decoded = decoder(batch['reviews_embedding'])\n",
        "    loss = criterion_c(decoded, batch['data'])\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_de.step()\n",
        "    return loss.item()\n",
        "  \n",
        "\n",
        "  def eval_function(engine, batch):\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        decoded = decoder(batch['reviews_embedding'])\n",
        "        return decoded, batch['data']\n",
        "  \n",
        "  trainer = Engine(process_function)\n",
        "  train_evaluator = Engine(eval_function)\n",
        "  validation_evaluator = Engine(eval_function)\n",
        "\n",
        "  metric = Loss(criterion_c)\n",
        "  metric.attach(train_evaluator, 'loss')\n",
        "  metric.attach(validation_evaluator, 'loss')\n",
        "\n",
        "  training_losses = []\n",
        "  validation_losses = []\n",
        "\n",
        "  @trainer.on(Events.EPOCH_COMPLETED)\n",
        "  def log_training_results(engine):\n",
        "      train_evaluator.run(train_iterator)\n",
        "      metrics = train_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']    \n",
        "      training_losses.append(avg_loss)\n",
        "      print(\"Training Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "           .format(engine.state.epoch, avg_loss))\n",
        "      \n",
        "  def log_validation_results(engine):\n",
        "      validation_evaluator.run(valid_iterator)\n",
        "      metrics = validation_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']\n",
        "      validation_losses.append(avg_loss)\n",
        "\n",
        "      print(\"Validation Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "            .format(engine.state.epoch, avg_loss))\n",
        "\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\n",
        "\n",
        "  # Reduce on Plateau\n",
        "  def average_loss(engine):\n",
        "    print(\"Current lr: {}\".format(optimizer_de.param_groups[0]['lr']))\n",
        "    average_loss = engine.state.metrics['loss']\n",
        "    scheduler_de.step(average_loss)\n",
        "\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, average_loss)\n",
        "  \n",
        "  # Early Stopping\n",
        "  def score_function(engine):\n",
        "      val_loss = engine.state.metrics['loss']\n",
        "      return -val_loss\n",
        "\n",
        "  handler = EarlyStopping(patience=30, score_function=score_function, trainer=trainer)\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n",
        "\n",
        "  # Model Checkpoint\n",
        "  checkpointer = ModelCheckpoint(str(DRIVE_PATH.joinpath('models')), name, n_saved=1, create_dir=False, save_as_state_dict=True, require_empty=False)\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'decoder_c': decoder})\n",
        "\n",
        "  train_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(train_indices))\n",
        "  valid_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(test_indices))\n",
        "\n",
        "  trainer.run(train_iterator, max_epochs=1000)\n",
        "  return training_losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0G6Xm--bxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delta_embedding = encoder(reviews[2016].data) - encoder(reviews[2011].data)\n",
        "delta_census = educations[2016].values - educations[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N13NFMvWJZRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [educations[2011].shape[1], pca_components]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvBlIbgHOMbH",
        "colab_type": "code",
        "outputId": "01c3bc2c-cbb2-4851-a04b-b5e4f300b99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i])\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0479105313\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0482932726\n",
            "Training Results - Epoch: 2  Avg loss: 0.0213039659\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0223544575\n",
            "Training Results - Epoch: 3  Avg loss: 0.0076624275\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0086351263\n",
            "Training Results - Epoch: 4  Avg loss: 0.0024466911\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0032047574\n",
            "Training Results - Epoch: 5  Avg loss: 0.0011215215\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0017941820\n",
            "Training Results - Epoch: 6  Avg loss: 0.0008330206\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0013699129\n",
            "Training Results - Epoch: 7  Avg loss: 0.0007443391\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0011972743\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006784682\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0011219031\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006268584\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0010349812\n",
            "Training Results - Epoch: 10  Avg loss: 0.0005874338\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0009581546\n",
            "Training Results - Epoch: 11  Avg loss: 0.0005598825\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0009395100\n",
            "Training Results - Epoch: 12  Avg loss: 0.0005280613\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0008557688\n",
            "Training Results - Epoch: 13  Avg loss: 0.0005033555\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0008241048\n",
            "Training Results - Epoch: 14  Avg loss: 0.0004917472\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0008196910\n",
            "Training Results - Epoch: 15  Avg loss: 0.0004672150\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0007573392\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004498040\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0007175010\n",
            "Training Results - Epoch: 17  Avg loss: 0.0004451981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0006984296\n",
            "Training Results - Epoch: 18  Avg loss: 0.0004265176\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0006767888\n",
            "Training Results - Epoch: 19  Avg loss: 0.0004217234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0006480556\n",
            "Training Results - Epoch: 20  Avg loss: 0.0004123837\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0006609243\n",
            "Training Results - Epoch: 21  Avg loss: 0.0004023490\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0006384672\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003995237\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0006338862\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003926858\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0005980305\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003888407\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0005978841\n",
            "Training Results - Epoch: 25  Avg loss: 0.0003824851\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0005944352\n",
            "Training Results - Epoch: 26  Avg loss: 0.0003784081\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0005849630\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003783193\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0005765845\n",
            "Training Results - Epoch: 28  Avg loss: 0.0003738325\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0005473291\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003718732\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0005598268\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003657541\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0005399628\n",
            "Training Results - Epoch: 31  Avg loss: 0.0003652163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0005566389\n",
            "Training Results - Epoch: 32  Avg loss: 0.0003609830\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0005460820\n",
            "Training Results - Epoch: 33  Avg loss: 0.0003622257\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0005204857\n",
            "Training Results - Epoch: 34  Avg loss: 0.0003611645\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005443872\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003574307\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0005090319\n",
            "Training Results - Epoch: 36  Avg loss: 0.0003630609\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0005175123\n",
            "Training Results - Epoch: 37  Avg loss: 0.0003611074\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0005169455\n",
            "Training Results - Epoch: 38  Avg loss: 0.0003539827\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0005097410\n",
            "Training Results - Epoch: 39  Avg loss: 0.0003468554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0005096986\n",
            "Training Results - Epoch: 40  Avg loss: 0.0003587183\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0005066007\n",
            "Training Results - Epoch: 41  Avg loss: 0.0003477196\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0004928182\n",
            "Training Results - Epoch: 42  Avg loss: 0.0003505577\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0005156110\n",
            "Training Results - Epoch: 43  Avg loss: 0.0003444749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0005006084\n",
            "Training Results - Epoch: 44  Avg loss: 0.0003438477\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0005006350\n",
            "Training Results - Epoch: 45  Avg loss: 0.0003485875\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0005150302\n",
            "Training Results - Epoch: 46  Avg loss: 0.0003459886\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0005113695\n",
            "Training Results - Epoch: 47  Avg loss: 0.0003388888\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0005003461\n",
            "Training Results - Epoch: 48  Avg loss: 0.0003386684\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0004980182\n",
            "Training Results - Epoch: 49  Avg loss: 0.0003367019\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0004806321\n",
            "Training Results - Epoch: 50  Avg loss: 0.0003419671\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0004878838\n",
            "Training Results - Epoch: 51  Avg loss: 0.0003484130\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0004777685\n",
            "Training Results - Epoch: 52  Avg loss: 0.0003637409\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0005300213\n",
            "Training Results - Epoch: 53  Avg loss: 0.0003375005\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0004956158\n",
            "Training Results - Epoch: 54  Avg loss: 0.0003298495\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0004938525\n",
            "Training Results - Epoch: 55  Avg loss: 0.0003412804\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0004929273\n",
            "Training Results - Epoch: 56  Avg loss: 0.0003315111\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005043350\n",
            "Training Results - Epoch: 57  Avg loss: 0.0003425704\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0005179288\n",
            "Training Results - Epoch: 58  Avg loss: 0.0003368196\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0004978064\n",
            "Training Results - Epoch: 59  Avg loss: 0.0003510693\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0004767039\n",
            "Training Results - Epoch: 60  Avg loss: 0.0003432453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0005095172\n",
            "Training Results - Epoch: 61  Avg loss: 0.0003463379\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0004759704\n",
            "Training Results - Epoch: 62  Avg loss: 0.0003533521\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0005332479\n",
            "Training Results - Epoch: 63  Avg loss: 0.0003268912\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0004929233\n",
            "Training Results - Epoch: 64  Avg loss: 0.0003271458\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0004849018\n",
            "Training Results - Epoch: 65  Avg loss: 0.0003265910\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0004877317\n",
            "Training Results - Epoch: 66  Avg loss: 0.0003303358\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0004975489\n",
            "Training Results - Epoch: 67  Avg loss: 0.0003407496\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0005091498\n",
            "Training Results - Epoch: 68  Avg loss: 0.0003280883\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0004813737\n",
            "Training Results - Epoch: 69  Avg loss: 0.0003287830\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0005027217\n",
            "Training Results - Epoch: 70  Avg loss: 0.0003322818\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0004934414\n",
            "Training Results - Epoch: 71  Avg loss: 0.0003218433\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0004986493\n",
            "Training Results - Epoch: 72  Avg loss: 0.0003255580\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0004893366\n",
            "Training Results - Epoch: 73  Avg loss: 0.0003216773\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0004902188\n",
            "Training Results - Epoch: 74  Avg loss: 0.0003260823\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0004997403\n",
            "Training Results - Epoch: 75  Avg loss: 0.0003202811\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0005023442\n",
            "Training Results - Epoch: 76  Avg loss: 0.0003329778\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0005001607\n",
            "Training Results - Epoch: 77  Avg loss: 0.0003305371\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0005110476\n",
            "Training Results - Epoch: 78  Avg loss: 0.0003265078\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0004976882\n",
            "Training Results - Epoch: 79  Avg loss: 0.0003189104\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0004883582\n",
            "Training Results - Epoch: 80  Avg loss: 0.0003204013\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0005043390\n",
            "Training Results - Epoch: 81  Avg loss: 0.0003177761\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0004879624\n",
            "Training Results - Epoch: 82  Avg loss: 0.0003250326\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0005075577\n",
            "Training Results - Epoch: 83  Avg loss: 0.0003163730\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0004966481\n",
            "Training Results - Epoch: 84  Avg loss: 0.0003153250\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0004943474\n",
            "Training Results - Epoch: 85  Avg loss: 0.0003142590\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0004931164\n",
            "Training Results - Epoch: 86  Avg loss: 0.0003143452\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0004946542\n",
            "Training Results - Epoch: 87  Avg loss: 0.0003141501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0004927272\n",
            "Training Results - Epoch: 88  Avg loss: 0.0003141094\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0004938954\n",
            "Training Results - Epoch: 89  Avg loss: 0.0003140079\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0004912650\n",
            "Training Results - Epoch: 90  Avg loss: 0.0003142174\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0004899271\n",
            "Training Results - Epoch: 91  Avg loss: 0.0003139309\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0004922329\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0003918933\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0003222980\n",
            "Training Results - Epoch: 2  Avg loss: 0.0003972347\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0003203230\n",
            "Training Results - Epoch: 3  Avg loss: 0.0003741689\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002887226\n",
            "Training Results - Epoch: 4  Avg loss: 0.0003992331\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0003105569\n",
            "Training Results - Epoch: 5  Avg loss: 0.0003861903\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0003046032\n",
            "Training Results - Epoch: 6  Avg loss: 0.0003730922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002793131\n",
            "Training Results - Epoch: 7  Avg loss: 0.0003799681\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002939914\n",
            "Training Results - Epoch: 8  Avg loss: 0.0003665343\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002897442\n",
            "Training Results - Epoch: 9  Avg loss: 0.0003681352\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002857850\n",
            "Training Results - Epoch: 10  Avg loss: 0.0003753470\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002866814\n",
            "Training Results - Epoch: 11  Avg loss: 0.0003778272\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002975173\n",
            "Training Results - Epoch: 12  Avg loss: 0.0003835429\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002955678\n",
            "Training Results - Epoch: 13  Avg loss: 0.0003668372\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002910750\n",
            "Training Results - Epoch: 14  Avg loss: 0.0003765175\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002945901\n",
            "Training Results - Epoch: 15  Avg loss: 0.0003640768\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002854754\n",
            "Training Results - Epoch: 16  Avg loss: 0.0003697279\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003051311\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003685439\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003043924\n",
            "Training Results - Epoch: 18  Avg loss: 0.0003627600\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002921722\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003807670\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003134393\n",
            "Training Results - Epoch: 20  Avg loss: 0.0003701013\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003064107\n",
            "Training Results - Epoch: 21  Avg loss: 0.0003648202\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003082564\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003674336\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003113524\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003635802\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003181203\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003645555\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003265675\n",
            "Training Results - Epoch: 25  Avg loss: 0.0003688517\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0003221661\n",
            "Training Results - Epoch: 26  Avg loss: 0.0003748680\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0003399513\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003628117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003166027\n",
            "Training Results - Epoch: 28  Avg loss: 0.0003566776\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0003127196\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003560099\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003130090\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003551251\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003131627\n",
            "Training Results - Epoch: 31  Avg loss: 0.0003551052\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003138742\n",
            "Training Results - Epoch: 32  Avg loss: 0.0003550670\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003138244\n",
            "Training Results - Epoch: 33  Avg loss: 0.0003549597\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003143782\n",
            "Training Results - Epoch: 34  Avg loss: 0.0003548771\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003142226\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003549027\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003145190\n",
            "Training Results - Epoch: 36  Avg loss: 0.0003548326\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003152019\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0003045381\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0004481916\n",
            "Training Results - Epoch: 2  Avg loss: 0.0003128044\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0004889958\n",
            "Training Results - Epoch: 3  Avg loss: 0.0003073518\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0004665262\n",
            "Training Results - Epoch: 4  Avg loss: 0.0002980226\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0004562343\n",
            "Training Results - Epoch: 5  Avg loss: 0.0003220959\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0004818299\n",
            "Training Results - Epoch: 6  Avg loss: 0.0002899151\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0004625171\n",
            "Training Results - Epoch: 7  Avg loss: 0.0002899323\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0004742110\n",
            "Training Results - Epoch: 8  Avg loss: 0.0002849815\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0004769150\n",
            "Training Results - Epoch: 9  Avg loss: 0.0002951329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0004831489\n",
            "Training Results - Epoch: 10  Avg loss: 0.0002896788\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0004917304\n",
            "Training Results - Epoch: 11  Avg loss: 0.0002897362\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0004878128\n",
            "Training Results - Epoch: 12  Avg loss: 0.0002887680\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0004768146\n",
            "Training Results - Epoch: 13  Avg loss: 0.0002850070\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004860166\n",
            "Training Results - Epoch: 14  Avg loss: 0.0002833918\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0004968694\n",
            "Training Results - Epoch: 15  Avg loss: 0.0003045769\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0005346737\n",
            "Training Results - Epoch: 16  Avg loss: 0.0002935630\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005049970\n",
            "Training Results - Epoch: 17  Avg loss: 0.0002850505\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0004843642\n",
            "Training Results - Epoch: 18  Avg loss: 0.0002912551\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004945688\n",
            "Training Results - Epoch: 19  Avg loss: 0.0002820589\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0004976030\n",
            "Training Results - Epoch: 20  Avg loss: 0.0002813344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005042253\n",
            "Training Results - Epoch: 21  Avg loss: 0.0002856196\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0005038075\n",
            "Training Results - Epoch: 22  Avg loss: 0.0002884072\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005240457\n",
            "Training Results - Epoch: 23  Avg loss: 0.0002780612\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0005053625\n",
            "Training Results - Epoch: 24  Avg loss: 0.0002761959\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0005018614\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002752748\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004978838\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002751255\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0004973248\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002749762\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004962709\n",
            "Training Results - Epoch: 28  Avg loss: 0.0002749246\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0004949592\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002749329\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0004976014\n",
            "Training Results - Epoch: 30  Avg loss: 0.0002748407\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0004971919\n",
            "Training Results - Epoch: 31  Avg loss: 0.0002748090\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0004963465\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0003488504\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0003560910\n",
            "Training Results - Epoch: 2  Avg loss: 0.0003391383\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0003815508\n",
            "Training Results - Epoch: 3  Avg loss: 0.0003349305\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0003790907\n",
            "Training Results - Epoch: 4  Avg loss: 0.0003257314\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0003864228\n",
            "Training Results - Epoch: 5  Avg loss: 0.0003338808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0003885197\n",
            "Training Results - Epoch: 6  Avg loss: 0.0003340033\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0003861371\n",
            "Training Results - Epoch: 7  Avg loss: 0.0003448653\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0004177287\n",
            "Training Results - Epoch: 8  Avg loss: 0.0003256601\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0004050411\n",
            "Training Results - Epoch: 9  Avg loss: 0.0003216891\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0004103780\n",
            "Training Results - Epoch: 10  Avg loss: 0.0003151517\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0004116280\n",
            "Training Results - Epoch: 11  Avg loss: 0.0003217295\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0004127413\n",
            "Training Results - Epoch: 12  Avg loss: 0.0003135710\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0004159905\n",
            "Training Results - Epoch: 13  Avg loss: 0.0003195530\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004306022\n",
            "Training Results - Epoch: 14  Avg loss: 0.0003226038\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0004533171\n",
            "Training Results - Epoch: 15  Avg loss: 0.0003248682\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0004578735\n",
            "Training Results - Epoch: 16  Avg loss: 0.0003176089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0004306852\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003202808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0004409944\n",
            "Training Results - Epoch: 18  Avg loss: 0.0003349549\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004338611\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003185575\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0004522911\n",
            "Training Results - Epoch: 20  Avg loss: 0.0003307353\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0004969759\n",
            "Training Results - Epoch: 21  Avg loss: 0.0003316066\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0004459143\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003094044\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0004705147\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003057013\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004627087\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003051618\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0004620730\n",
            "Training Results - Epoch: 25  Avg loss: 0.0003048181\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004607384\n",
            "Training Results - Epoch: 26  Avg loss: 0.0003047710\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0004620983\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003046404\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004607635\n",
            "Training Results - Epoch: 28  Avg loss: 0.0003045999\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0004619748\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003045241\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0004653785\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003045173\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0004625143\n",
            "Training Results - Epoch: 31  Avg loss: 0.0003044545\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0004640867\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0003653312\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0003084095\n",
            "Training Results - Epoch: 2  Avg loss: 0.0003691921\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0003379982\n",
            "Training Results - Epoch: 3  Avg loss: 0.0003751222\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0003416319\n",
            "Training Results - Epoch: 4  Avg loss: 0.0003689722\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0003321559\n",
            "Training Results - Epoch: 5  Avg loss: 0.0003376210\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0003250741\n",
            "Training Results - Epoch: 6  Avg loss: 0.0003522055\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0003346490\n",
            "Training Results - Epoch: 7  Avg loss: 0.0003441306\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0003390268\n",
            "Training Results - Epoch: 8  Avg loss: 0.0003447609\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0003394809\n",
            "Training Results - Epoch: 9  Avg loss: 0.0003576613\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0003655506\n",
            "Training Results - Epoch: 10  Avg loss: 0.0003378164\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003538496\n",
            "Training Results - Epoch: 11  Avg loss: 0.0003314939\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003402658\n",
            "Training Results - Epoch: 12  Avg loss: 0.0003336297\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0003515094\n",
            "Training Results - Epoch: 13  Avg loss: 0.0003509449\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0003761360\n",
            "Training Results - Epoch: 14  Avg loss: 0.0003261758\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0003447847\n",
            "Training Results - Epoch: 15  Avg loss: 0.0003330809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003472093\n",
            "Training Results - Epoch: 16  Avg loss: 0.0003328865\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003681180\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003317613\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003650526\n",
            "Training Results - Epoch: 18  Avg loss: 0.0003232398\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0003547604\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003386123\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003812072\n",
            "Training Results - Epoch: 20  Avg loss: 0.0003250017\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003617760\n",
            "Training Results - Epoch: 21  Avg loss: 0.0003376060\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003770860\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003281384\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003813843\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003213778\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003673478\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003200425\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003632998\n",
            "Training Results - Epoch: 25  Avg loss: 0.0003197104\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0003624926\n",
            "Training Results - Epoch: 26  Avg loss: 0.0003195901\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0003628762\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003194351\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003614987\n",
            "Training Results - Epoch: 28  Avg loss: 0.0003193646\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0003616274\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003193709\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003612472\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003192880\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003611305\n",
            "Training Results - Epoch: 31  Avg loss: 0.0003192675\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003619929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVsY58vBYt1u",
        "colab_type": "code",
        "outputId": "3f5c4a63-4269-4e28-8352-37c8aa575558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.savefig(DRIVE_PATH.joinpath('loss.png'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSLq6fd85AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VlJwKMyxKzY",
        "colab_type": "code",
        "outputId": "3e4ab053-25f1-402e-b992-1b9d1be1a8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cMq3lkm-zGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "actual_data = educations[2016].values\n",
        "predicted_data = educations[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Sp-VUP0dDe",
        "colab_type": "code",
        "outputId": "a569a7d7-fc05-4994-b1b1-6b832ff17467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08216523946922268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Pnj6AV8jXh",
        "colab_type": "code",
        "outputId": "56cae389-947d-473b-fc90-2e7f33d1cab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09326672611484385"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc7Acl_o_q8Y",
        "colab_type": "text"
      },
      "source": [
        "## ELMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBVwfdBT_xAg",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model and built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNGkUzM_9Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_elmo_all = {}\n",
        "reviews_elmo = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_elmo_all[year] = pd.read_csv(DRIVE_PATH.joinpath('elmo_reviews_{}.csv'.format(year)))\n",
        "  reviews_elmo_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "  reviews_elmo[year] = ReviewsVector(reviews_elmo_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkrab33AUoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delta_embedding = reviews_elmo[2016].data.T - reviews_elmo[2011].data.T\n",
        "delta_census = educations[2016].values - educations[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtFEv6GZAMBR",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK-0krFfAPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lingreg = RidgeCV(cv=5)\n",
        "lingreg.fit(delta_embedding[train_val_neighbourhoods].detach().cpu().numpy(), delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDvok7RlO9Te",
        "colab_type": "code",
        "outputId": "f1c2495a-17f9-43a0-9a65-def4f169428e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08013128499367751"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjkWFcjRO-Lv",
        "colab_type": "code",
        "outputId": "72ba5af7-df55-4163-fa76-e28da07f92cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08077842308263497"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-NPDm4AooY",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUFcqugAwjZ",
        "colab_type": "text"
      },
      "source": [
        "#### With one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wksmfZieA1ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3OvtNXeA2mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [educations[2011].shape[1], reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NssE1sABNrK3",
        "colab_type": "code",
        "outputId": "8442a9f2-60bd-4ee6-d4e4-bdf04bfdbd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='edu_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0008506472\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0009594537\n",
            "Training Results - Epoch: 2  Avg loss: 0.0010971311\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0024918874\n",
            "Training Results - Epoch: 3  Avg loss: 0.0014217979\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0027997126\n",
            "Training Results - Epoch: 4  Avg loss: 0.0008632540\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0016792136\n",
            "Training Results - Epoch: 5  Avg loss: 0.0009406624\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0010418944\n",
            "Training Results - Epoch: 6  Avg loss: 0.0006621630\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0010999716\n",
            "Training Results - Epoch: 7  Avg loss: 0.0020066261\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0021902202\n",
            "Training Results - Epoch: 8  Avg loss: 0.0007794868\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0014872591\n",
            "Training Results - Epoch: 9  Avg loss: 0.0026603351\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0039008969\n",
            "Training Results - Epoch: 10  Avg loss: 0.0010347822\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0011582275\n",
            "Training Results - Epoch: 11  Avg loss: 0.0009528224\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0011191842\n",
            "Training Results - Epoch: 12  Avg loss: 0.0016718010\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0017399139\n",
            "Training Results - Epoch: 13  Avg loss: 0.0011924760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0018717653\n",
            "Training Results - Epoch: 14  Avg loss: 0.0009199748\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0012813612\n",
            "Training Results - Epoch: 15  Avg loss: 0.0002762053\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0005850835\n",
            "Training Results - Epoch: 16  Avg loss: 0.0002490831\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0007203258\n",
            "Training Results - Epoch: 17  Avg loss: 0.0004331842\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0009279887\n",
            "Training Results - Epoch: 18  Avg loss: 0.0002858882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0008137197\n",
            "Training Results - Epoch: 19  Avg loss: 0.0002858745\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0007109403\n",
            "Training Results - Epoch: 20  Avg loss: 0.0009421619\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0012524614\n",
            "Training Results - Epoch: 21  Avg loss: 0.0002744838\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0006503501\n",
            "Training Results - Epoch: 22  Avg loss: 0.0022154393\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0050577916\n",
            "Training Results - Epoch: 23  Avg loss: 0.0008327547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0021710757\n",
            "Training Results - Epoch: 24  Avg loss: 0.0020861200\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0049203522\n",
            "Training Results - Epoch: 25  Avg loss: 0.0010177968\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0043370235\n",
            "Training Results - Epoch: 26  Avg loss: 0.0006927473\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0019688340\n",
            "Training Results - Epoch: 27  Avg loss: 0.0044245547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0059651742\n",
            "Training Results - Epoch: 28  Avg loss: 0.0028637687\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0061588787\n",
            "Training Results - Epoch: 29  Avg loss: 0.0012865594\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0014836257\n",
            "Training Results - Epoch: 30  Avg loss: 0.0006767221\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0016736392\n",
            "Training Results - Epoch: 31  Avg loss: 0.0008810896\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0009716124\n",
            "Training Results - Epoch: 32  Avg loss: 0.0011030081\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0022586821\n",
            "Training Results - Epoch: 33  Avg loss: 0.0020900749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0037614464\n",
            "Training Results - Epoch: 34  Avg loss: 0.0020298896\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0039221238\n",
            "Training Results - Epoch: 35  Avg loss: 0.0006507206\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0010765463\n",
            "Training Results - Epoch: 36  Avg loss: 0.0002975305\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0009077113\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002209062\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0006667191\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002060111\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0006925295\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001894714\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0006110287\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001695546\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0006394512\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001641613\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0006664986\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001545644\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0006488841\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001498608\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0006545102\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001457796\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0006239254\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001426778\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0006361188\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0039443711\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0008746475\n",
            "Training Results - Epoch: 2  Avg loss: 0.0023006816\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0012108971\n",
            "Training Results - Epoch: 3  Avg loss: 0.0025362268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0003302818\n",
            "Training Results - Epoch: 4  Avg loss: 0.0043870351\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0008566685\n",
            "Training Results - Epoch: 5  Avg loss: 0.0040779931\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0010718388\n",
            "Training Results - Epoch: 6  Avg loss: 0.0031848509\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0009138545\n",
            "Training Results - Epoch: 7  Avg loss: 0.0033921763\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0006015863\n",
            "Training Results - Epoch: 8  Avg loss: 0.0035329501\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0010501253\n",
            "Training Results - Epoch: 9  Avg loss: 0.0012400219\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0008295884\n",
            "Training Results - Epoch: 10  Avg loss: 0.0026587753\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0006091152\n",
            "Training Results - Epoch: 11  Avg loss: 0.0039140344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003896508\n",
            "Training Results - Epoch: 12  Avg loss: 0.0021970115\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0005262378\n",
            "Training Results - Epoch: 13  Avg loss: 0.0027684844\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0003680516\n",
            "Training Results - Epoch: 14  Avg loss: 0.0023864856\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0004917912\n",
            "Training Results - Epoch: 15  Avg loss: 0.0032366043\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0023664991\n",
            "Training Results - Epoch: 16  Avg loss: 0.0012113772\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003835249\n",
            "Training Results - Epoch: 17  Avg loss: 0.0019576267\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003065834\n",
            "Training Results - Epoch: 18  Avg loss: 0.0045073098\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0008153883\n",
            "Training Results - Epoch: 19  Avg loss: 0.0035446055\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003963938\n",
            "Training Results - Epoch: 20  Avg loss: 0.0023876984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0010516898\n",
            "Training Results - Epoch: 21  Avg loss: 0.0104159912\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0038996057\n",
            "Training Results - Epoch: 22  Avg loss: 0.0041075604\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0007798575\n",
            "Training Results - Epoch: 23  Avg loss: 0.0036123135\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0029045192\n",
            "Training Results - Epoch: 24  Avg loss: 0.0056761488\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0012125958\n",
            "Training Results - Epoch: 25  Avg loss: 0.0022486930\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0005477621\n",
            "Training Results - Epoch: 26  Avg loss: 0.0030031057\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0015748884\n",
            "Training Results - Epoch: 27  Avg loss: 0.0032585759\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004503718\n",
            "Training Results - Epoch: 28  Avg loss: 0.0024058153\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0005225806\n",
            "Training Results - Epoch: 29  Avg loss: 0.0032585041\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0007192839\n",
            "Training Results - Epoch: 30  Avg loss: 0.0028880305\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0007693117\n",
            "Training Results - Epoch: 31  Avg loss: 0.0044260873\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0007313021\n",
            "Training Results - Epoch: 32  Avg loss: 0.0018877440\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003573319\n",
            "Training Results - Epoch: 33  Avg loss: 0.0047353872\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0011411013\n",
            "Training Results - Epoch: 34  Avg loss: 0.0035202654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0015466338\n",
            "Training Results - Epoch: 35  Avg loss: 0.0031836809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0007644254\n",
            "Training Results - Epoch: 36  Avg loss: 0.0036207808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0031100862\n",
            "Training Results - Epoch: 37  Avg loss: 0.0039298119\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0007075215\n",
            "Training Results - Epoch: 38  Avg loss: 0.0072914589\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0021726783\n",
            "Training Results - Epoch: 39  Avg loss: 0.0035871433\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0010929383\n",
            "Training Results - Epoch: 40  Avg loss: 0.0011834501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0011149232\n",
            "Training Results - Epoch: 41  Avg loss: 0.0006634888\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0006816514\n",
            "Training Results - Epoch: 42  Avg loss: 0.0005528272\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0005455451\n",
            "Training Results - Epoch: 43  Avg loss: 0.0002659437\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0004521903\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002147024\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0004159241\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001781859\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003647664\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001644816\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0003451256\n",
            "Training Results - Epoch: 47  Avg loss: 0.0001572792\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0003313389\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0011628338\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0010983936\n",
            "Training Results - Epoch: 2  Avg loss: 0.0009710122\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0016683766\n",
            "Training Results - Epoch: 3  Avg loss: 0.0010248636\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0006876543\n",
            "Training Results - Epoch: 4  Avg loss: 0.0006267077\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0010490020\n",
            "Training Results - Epoch: 5  Avg loss: 0.0008496065\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0025090647\n",
            "Training Results - Epoch: 6  Avg loss: 0.0012152459\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0016071276\n",
            "Training Results - Epoch: 7  Avg loss: 0.0009812828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0018572532\n",
            "Training Results - Epoch: 8  Avg loss: 0.0003674030\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0007957769\n",
            "Training Results - Epoch: 9  Avg loss: 0.0002180652\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0003894869\n",
            "Training Results - Epoch: 10  Avg loss: 0.0002633472\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003314563\n",
            "Training Results - Epoch: 11  Avg loss: 0.0010429155\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0019787241\n",
            "Training Results - Epoch: 12  Avg loss: 0.0004036230\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0007472711\n",
            "Training Results - Epoch: 13  Avg loss: 0.0006366569\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0015476685\n",
            "Training Results - Epoch: 14  Avg loss: 0.0002142163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0004736805\n",
            "Training Results - Epoch: 15  Avg loss: 0.0003671157\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0004432601\n",
            "Training Results - Epoch: 16  Avg loss: 0.0005271386\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0008702173\n",
            "Training Results - Epoch: 17  Avg loss: 0.0011458559\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0014287599\n",
            "Training Results - Epoch: 18  Avg loss: 0.0021568354\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0028776526\n",
            "Training Results - Epoch: 19  Avg loss: 0.0013104282\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0019330942\n",
            "Training Results - Epoch: 20  Avg loss: 0.0012383227\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0013584153\n",
            "Training Results - Epoch: 21  Avg loss: 0.0010081872\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0012923938\n",
            "Training Results - Epoch: 22  Avg loss: 0.0005427213\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0008508787\n",
            "Training Results - Epoch: 23  Avg loss: 0.0009806489\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0037674487\n",
            "Training Results - Epoch: 24  Avg loss: 0.0009327975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0019096148\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002582686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004806517\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002268708\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0005520219\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002861103\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0005956913\n",
            "Training Results - Epoch: 28  Avg loss: 0.0003193032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0005509150\n",
            "Training Results - Epoch: 29  Avg loss: 0.0005251358\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0007870014\n",
            "Training Results - Epoch: 30  Avg loss: 0.0005233267\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0005353371\n",
            "Training Results - Epoch: 31  Avg loss: 0.0004139067\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0006981471\n",
            "Training Results - Epoch: 32  Avg loss: 0.0002355751\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003600057\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001912427\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003492035\n",
            "Training Results - Epoch: 34  Avg loss: 0.0001504497\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003876076\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001485513\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0004727791\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001215951\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003468529\n",
            "Training Results - Epoch: 37  Avg loss: 0.0001103369\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003833047\n",
            "Training Results - Epoch: 38  Avg loss: 0.0001133907\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003448903\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001026423\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003529775\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000998306\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003955163\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0029084032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0011057780\n",
            "Training Results - Epoch: 2  Avg loss: 0.0034778268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0004298419\n",
            "Training Results - Epoch: 3  Avg loss: 0.0051660415\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0041474371\n",
            "Training Results - Epoch: 4  Avg loss: 0.0037210698\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0027422750\n",
            "Training Results - Epoch: 5  Avg loss: 0.0018677766\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002505297\n",
            "Training Results - Epoch: 6  Avg loss: 0.0030400404\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0023593602\n",
            "Training Results - Epoch: 7  Avg loss: 0.0041718359\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0012894263\n",
            "Training Results - Epoch: 8  Avg loss: 0.0039847467\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0005257132\n",
            "Training Results - Epoch: 9  Avg loss: 0.0042283810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0010284643\n",
            "Training Results - Epoch: 10  Avg loss: 0.0097946056\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0052789775\n",
            "Training Results - Epoch: 11  Avg loss: 0.0041614144\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0006296540\n",
            "Training Results - Epoch: 12  Avg loss: 0.0039169616\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0010002848\n",
            "Training Results - Epoch: 13  Avg loss: 0.0028181624\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004635745\n",
            "Training Results - Epoch: 14  Avg loss: 0.0093803496\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0053708753\n",
            "Training Results - Epoch: 15  Avg loss: 0.0030259825\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003621929\n",
            "Training Results - Epoch: 16  Avg loss: 0.0064690064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0048375243\n",
            "Training Results - Epoch: 17  Avg loss: 0.0025618850\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0004444259\n",
            "Training Results - Epoch: 18  Avg loss: 0.0037261070\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0016294715\n",
            "Training Results - Epoch: 19  Avg loss: 0.0040154809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0016834338\n",
            "Training Results - Epoch: 20  Avg loss: 0.0038414341\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0007872333\n",
            "Training Results - Epoch: 21  Avg loss: 0.0040679731\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0006836840\n",
            "Training Results - Epoch: 22  Avg loss: 0.0065415025\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0030873877\n",
            "Training Results - Epoch: 23  Avg loss: 0.0031784567\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002818370\n",
            "Training Results - Epoch: 24  Avg loss: 0.0038994365\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0011007970\n",
            "Training Results - Epoch: 25  Avg loss: 0.0045728880\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0007652873\n",
            "Training Results - Epoch: 26  Avg loss: 0.0052255315\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0017365191\n",
            "Training Results - Epoch: 27  Avg loss: 0.0033711925\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0010700982\n",
            "Training Results - Epoch: 28  Avg loss: 0.0027548454\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0007821073\n",
            "Training Results - Epoch: 29  Avg loss: 0.0012057762\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0005415914\n",
            "Training Results - Epoch: 30  Avg loss: 0.0006802749\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003516086\n",
            "Training Results - Epoch: 31  Avg loss: 0.0004315069\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002924392\n",
            "Training Results - Epoch: 32  Avg loss: 0.0002890729\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0002462816\n",
            "Training Results - Epoch: 33  Avg loss: 0.0002130581\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002194835\n",
            "Training Results - Epoch: 34  Avg loss: 0.0001787128\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002065125\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001634795\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0002003457\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001525176\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0001994817\n",
            "Training Results - Epoch: 37  Avg loss: 0.0001443151\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0001938086\n",
            "Training Results - Epoch: 38  Avg loss: 0.0001381403\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0001912970\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001339576\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0001920110\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001289827\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0001893558\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001253985\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0001867539\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001215697\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0001864532\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001180763\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0001840657\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001155013\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0001831985\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001132511\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0001819817\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001107256\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0001819291\n",
            "Training Results - Epoch: 47  Avg loss: 0.0001089940\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0001839495\n",
            "Training Results - Epoch: 48  Avg loss: 0.0001072378\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0001837345\n",
            "Training Results - Epoch: 49  Avg loss: 0.0001064902\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0001836896\n",
            "Training Results - Epoch: 50  Avg loss: 0.0001031454\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0001793647\n",
            "Training Results - Epoch: 51  Avg loss: 0.0001023582\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001805548\n",
            "Training Results - Epoch: 52  Avg loss: 0.0001003553\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001801327\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000992152\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001793117\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000978273\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001792285\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000970104\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001797035\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000960476\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001799070\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000958916\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001757994\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000950366\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001775563\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000940472\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001810225\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000928906\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001803668\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000918817\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001780338\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000917890\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001779400\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000909727\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001777878\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000911281\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001772350\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000914576\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001802155\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000899184\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001775968\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000886807\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001770206\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000899026\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0001803941\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000894944\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0001759523\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000883981\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001764098\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000904036\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0001793623\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000874367\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0001762202\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000891420\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0001802275\n",
            "Training Results - Epoch: 74  Avg loss: 0.0001226748\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0001939268\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000934689\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0001803196\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000896706\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001753685\n",
            "Training Results - Epoch: 77  Avg loss: 0.0001020843\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0001861394\n",
            "Training Results - Epoch: 78  Avg loss: 0.0001024332\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001822262\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001152818\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0001948598\n",
            "Training Results - Epoch: 80  Avg loss: 0.0001329633\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002025565\n",
            "Training Results - Epoch: 81  Avg loss: 0.0001208761\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0001827218\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000971389\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0001834434\n",
            "Training Results - Epoch: 83  Avg loss: 0.0001282325\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0001833567\n",
            "Training Results - Epoch: 84  Avg loss: 0.0001240922\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0001951440\n",
            "Training Results - Epoch: 85  Avg loss: 0.0001429768\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0001885719\n",
            "Training Results - Epoch: 86  Avg loss: 0.0001435188\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0001789142\n",
            "Training Results - Epoch: 87  Avg loss: 0.0003027863\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0002587505\n",
            "Training Results - Epoch: 88  Avg loss: 0.0002069612\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0001905440\n",
            "Training Results - Epoch: 89  Avg loss: 0.0001359619\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0002004714\n",
            "Training Results - Epoch: 90  Avg loss: 0.0001123293\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0002109981\n",
            "Training Results - Epoch: 91  Avg loss: 0.0001197165\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0001951888\n",
            "Training Results - Epoch: 92  Avg loss: 0.0001005019\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0001810703\n",
            "Training Results - Epoch: 93  Avg loss: 0.0001643328\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0001976061\n",
            "Training Results - Epoch: 94  Avg loss: 0.0001088178\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0001833818\n",
            "Training Results - Epoch: 95  Avg loss: 0.0001040117\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0001858535\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000912989\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0001777175\n",
            "Training Results - Epoch: 97  Avg loss: 0.0001080153\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0001800051\n",
            "Training Results - Epoch: 98  Avg loss: 0.0000857813\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 98  Avg loss: 0.0001771065\n",
            "Training Results - Epoch: 99  Avg loss: 0.0000812804\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 99  Avg loss: 0.0001796101\n",
            "Training Results - Epoch: 100  Avg loss: 0.0000805866\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 100  Avg loss: 0.0001806904\n",
            "Training Results - Epoch: 101  Avg loss: 0.0000799283\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 101  Avg loss: 0.0001816848\n",
            "Training Results - Epoch: 102  Avg loss: 0.0000797267\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 102  Avg loss: 0.0001815147\n",
            "Training Results - Epoch: 103  Avg loss: 0.0000795515\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 103  Avg loss: 0.0001822349\n",
            "Training Results - Epoch: 104  Avg loss: 0.0000794210\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 104  Avg loss: 0.0001818271\n",
            "Training Results - Epoch: 105  Avg loss: 0.0000793323\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 105  Avg loss: 0.0001818458\n",
            "Training Results - Epoch: 106  Avg loss: 0.0000792812\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 106  Avg loss: 0.0001812132\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0005952650\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0006941560\n",
            "Training Results - Epoch: 2  Avg loss: 0.0016324582\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0038824908\n",
            "Training Results - Epoch: 3  Avg loss: 0.0016642475\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0007999951\n",
            "Training Results - Epoch: 4  Avg loss: 0.0011091042\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0030612742\n",
            "Training Results - Epoch: 5  Avg loss: 0.0003027620\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0006105273\n",
            "Training Results - Epoch: 6  Avg loss: 0.0006726330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0007379939\n",
            "Training Results - Epoch: 7  Avg loss: 0.0014221527\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0017975186\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006517874\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0003885474\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006432894\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0014447111\n",
            "Training Results - Epoch: 10  Avg loss: 0.0006631550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003464118\n",
            "Training Results - Epoch: 11  Avg loss: 0.0007827259\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0011213911\n",
            "Training Results - Epoch: 12  Avg loss: 0.0008477170\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0009721929\n",
            "Training Results - Epoch: 13  Avg loss: 0.0008955332\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0009966187\n",
            "Training Results - Epoch: 14  Avg loss: 0.0008479930\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0008457055\n",
            "Training Results - Epoch: 15  Avg loss: 0.0019186745\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0020008998\n",
            "Training Results - Epoch: 16  Avg loss: 0.0016842174\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0023765036\n",
            "Training Results - Epoch: 17  Avg loss: 0.0016525954\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0010324209\n",
            "Training Results - Epoch: 18  Avg loss: 0.0004262832\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0003788497\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003508223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0004146214\n",
            "Training Results - Epoch: 20  Avg loss: 0.0004625691\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005913692\n",
            "Training Results - Epoch: 21  Avg loss: 0.0004889458\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0005947824\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003599772\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003408725\n",
            "Training Results - Epoch: 23  Avg loss: 0.0009265010\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0011279998\n",
            "Training Results - Epoch: 24  Avg loss: 0.0009174287\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0013559094\n",
            "Training Results - Epoch: 25  Avg loss: 0.0021686831\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0018594492\n",
            "Training Results - Epoch: 26  Avg loss: 0.0006475559\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0007326464\n",
            "Training Results - Epoch: 27  Avg loss: 0.0004364919\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003076528\n",
            "Training Results - Epoch: 28  Avg loss: 0.0008070629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0011693298\n",
            "Training Results - Epoch: 29  Avg loss: 0.0005402682\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0007728255\n",
            "Training Results - Epoch: 30  Avg loss: 0.0034994233\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0037316526\n",
            "Training Results - Epoch: 31  Avg loss: 0.0009636596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0005410858\n",
            "Training Results - Epoch: 32  Avg loss: 0.0018750698\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0015377621\n",
            "Training Results - Epoch: 33  Avg loss: 0.0013225086\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0006949793\n",
            "Training Results - Epoch: 34  Avg loss: 0.0014311495\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0012455172\n",
            "Training Results - Epoch: 35  Avg loss: 0.0009830029\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0007695450\n",
            "Training Results - Epoch: 36  Avg loss: 0.0005388993\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0004480653\n",
            "Training Results - Epoch: 37  Avg loss: 0.0004228779\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0004972675\n",
            "Training Results - Epoch: 38  Avg loss: 0.0006554989\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0006243537\n",
            "Training Results - Epoch: 39  Avg loss: 0.0004333625\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0004304065\n",
            "Training Results - Epoch: 40  Avg loss: 0.0004267018\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0004404979\n",
            "Training Results - Epoch: 41  Avg loss: 0.0003132504\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003258191\n",
            "Training Results - Epoch: 42  Avg loss: 0.0002134608\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003133628\n",
            "Training Results - Epoch: 43  Avg loss: 0.0002415111\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002680767\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001901292\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002650724\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001777812\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0002075599\n",
            "Training Results - Epoch: 46  Avg loss: 0.0002419078\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0004512694\n",
            "Training Results - Epoch: 47  Avg loss: 0.0003801612\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0006559647\n",
            "Training Results - Epoch: 48  Avg loss: 0.0008924444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0011820584\n",
            "Training Results - Epoch: 49  Avg loss: 0.0021446739\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0016392549\n",
            "Training Results - Epoch: 50  Avg loss: 0.0065877785\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0097316951\n",
            "Training Results - Epoch: 51  Avg loss: 0.0088797569\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0091899938\n",
            "Training Results - Epoch: 52  Avg loss: 0.0009096717\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0013076350\n",
            "Training Results - Epoch: 53  Avg loss: 0.0005446847\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0012752608\n",
            "Training Results - Epoch: 54  Avg loss: 0.0003122304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0005494144\n",
            "Training Results - Epoch: 55  Avg loss: 0.0004246199\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0003407306\n",
            "Training Results - Epoch: 56  Avg loss: 0.0001721596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0003919244\n",
            "Training Results - Epoch: 57  Avg loss: 0.0004617363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0009836421\n",
            "Training Results - Epoch: 58  Avg loss: 0.0002902488\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0005207784\n",
            "Training Results - Epoch: 59  Avg loss: 0.0001916513\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0002993989\n",
            "Training Results - Epoch: 60  Avg loss: 0.0003046920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0010437908\n",
            "Training Results - Epoch: 61  Avg loss: 0.0005408627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0006128937\n",
            "Training Results - Epoch: 62  Avg loss: 0.0003802922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0005431723\n",
            "Training Results - Epoch: 63  Avg loss: 0.0022622134\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0023882564\n",
            "Training Results - Epoch: 64  Avg loss: 0.0006485474\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0010605406\n",
            "Training Results - Epoch: 65  Avg loss: 0.0026958322\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0023934315\n",
            "Training Results - Epoch: 66  Avg loss: 0.0025876308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0050110191\n",
            "Training Results - Epoch: 67  Avg loss: 0.0007728101\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0011275592\n",
            "Training Results - Epoch: 68  Avg loss: 0.0006393801\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0003761634\n",
            "Training Results - Epoch: 69  Avg loss: 0.0004274702\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0004580541\n",
            "Training Results - Epoch: 70  Avg loss: 0.0003477857\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0003765759\n",
            "Training Results - Epoch: 71  Avg loss: 0.0003211481\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0004765767\n",
            "Training Results - Epoch: 72  Avg loss: 0.0002784188\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0004853064\n",
            "Training Results - Epoch: 73  Avg loss: 0.0002072985\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002973606\n",
            "Training Results - Epoch: 74  Avg loss: 0.0001784180\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0003476180\n",
            "Training Results - Epoch: 75  Avg loss: 0.0001739971\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002601071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbmY8XxTBIuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = educations[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p60UxdAkPsj4",
        "colab_type": "code",
        "outputId": "9c0d8c60-9d5c-42fe-e371-7823ebdfa3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05619282123431013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIFWfuGxPt15",
        "colab_type": "code",
        "outputId": "f8fa3636-cbbf-440c-c242-6ba355db31bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12545452057365256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJXQIgaBZav",
        "colab_type": "text"
      },
      "source": [
        "#### With Additional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPwMX2LsBuoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [educations[2011].shape[1], reviews_elmo_all[2016].shape[1]//2, reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOBwpUnlPL7T",
        "colab_type": "code",
        "outputId": "c6f4182d-392e-4bf6-90e8-2e2d5725f89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=512, out_features=6, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Wt9cjDBzxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='edu_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5RM5xCQBeJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = educations[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9c0d8c60-9d5c-42fe-e371-7823ebdfa3e4",
        "id": "PJO-Vs__BhP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05619282123431013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKVsosH0Ok-U",
        "colab_type": "code",
        "outputId": "0722e9f1-43c4-42e1-b91f-e96c2796180d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09251840091242075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8bf-trrJvmj",
        "colab_type": "text"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ30mPWTJxmL",
        "colab_type": "text"
      },
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th rowspan=2>Model</th>\n",
        "            <th rowspan=2> \"No change\" </th>\n",
        "            <th colspan=2>TF-IDF Autoencoder</th>\n",
        "            <th colspan=2>ELMO</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>NLR2</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>Training MAE</td>\n",
        "            <td>11.92%</td>\n",
        "            <td>7.94%</td>\n",
        "            <td>8.22%</td>\n",
        "            <td>8.01%</td>\n",
        "            <td>5.62%</td>\n",
        "            <td>5.62%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Testing MAE</td>\n",
        "            <td>12.11%</td>\n",
        "            <td>10.07%</td>\n",
        "            <td>9.33%</td>\n",
        "            <td>8.01%</td>\n",
        "            <td>12.55%</td>\n",
        "            <td>9.25%</td>\n",
        "        </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    }
  ]
}